{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view-in-github"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import for text analytics\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "import multiprocessing\n",
    "\n",
    "# Import libraries for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load English language model of spacy\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics 2: Word Embedding\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/0*QXJDYJTGexmpeQ43.jpg' width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onO46LysT2dh"
   },
   "source": [
    "## Content\n",
    "\n",
    "In this walkthrough, we pursue our exploration of [Text Analytics](https://en.wikipedia.org/wiki/Text_mining), diving into [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding) and doing an application on sentiment classification.\n",
    "\n",
    "- [Recap on text representation](#Recap-on-text-representation)\n",
    "    - [Some definitions](#Some-definitions)\n",
    "    - [Bag of Words (BOW)](#Bag-of-Words-(BOW))\n",
    "    - [TF-IDF](#TF-IDF)\n",
    "- [Introduction to Gensim and Word Embedding](#Introduction-to-Gensim-and-Word-Embedding)\n",
    "    - [Background](#Background)\n",
    "    - [Implementing Word2vec with Gensim](#Implementing-Word2vec-with-Gensim)\n",
    "    - [Your turn](#Your-turn)\n",
    "- [Application: Text Classification with TF-IDF vs Doc2Vec](#Application:-Text-Classification-with-TF-IDF-vs-Doc2Vec)\n",
    "    - [Load and clean data](#Load-and-clean-data)\n",
    "    - [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "    - [Classification using TF-IDF and Logistic Regression](#Classification-using-TF-IDF-and-Logistic-Regression)\n",
    "    - [Classification using Doc2Vec and Logistic Regression](#Classification-using-Doc2Vec-and-Logistic-Regression)\n",
    "    - [How to improve the accuracy of a text classifier?](#How-to-improve-the-accuracy-of-a-text-classifier?)\n",
    "- [Further reading](#Further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0VsVUPLT4Tn"
   },
   "source": [
    "## Recap on text representation\n",
    "\n",
    "In order to be able to use texts as inputs for classification, we have to transform them into numbers (i.e., vectors). There are several ways of doing this. We recap in this section the concepts and techniques seen last week, namely Bag of Words and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q95eek-OIIPq"
   },
   "source": [
    "### Some definitions\n",
    "\n",
    "- Document = some text, i.e., a string (e.g., a sentence, a tweet, paragraph of text, book, news article, etc.).\n",
    "- Corpus = collection of documents.\n",
    "- Dictionary = list of unique tokens in (preprocessed) corpus.\n",
    "- Vector = mathematical representation of a document (e.g., Bag of Words).\n",
    "- Model = algorithm used for transforming vectors from one representation to another (e.g., TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate these concepts in Python, using text from the article *[The decarbonisation of Europe powered by lifestyle changes](https://iopscience.iop.org/article/10.1088/1748-9326/abe890/meta)*.\n",
    "\n",
    "Reference: Costa, L., Moreau, V., et al. (2021). The decarbonisation of Europe powered by lifestyle changes. *Environmental Research Letters*, 16(4), 044057."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you can define a string using single quotes, double quotes, or triple quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ0TKUegLawc"
   },
   "outputs": [],
   "source": [
    "# A document\n",
    "doc = 'Changes in behaviour may contribute more than 20% of the GHG emission reductions required for net-zero.' \n",
    "doc = \"Changes in behaviour may contribute more than 20% of the GHG emission reductions required for net-zero.\" \n",
    "doc = \"\"\"Changes in behaviour may contribute more than 20% of the GHG emission reductions required for net-zero.\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a corpus, containing a collection of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jZsQ-LLbIG",
    "outputId": "0a16c681-7e48-46ec-cfc4-9d649d588793"
   },
   "outputs": [],
   "source": [
    "# A corpus\n",
    "d1 = \"The impacts of behavioural change vary across sectors.\"\n",
    "d2 = \"Changes in travel behaviour limit the rising demand for electricity.\"\n",
    "d3 = \"Adopting a healthy diet reduces emissions substantially.\"\n",
    "d4 = \"Without behavioural change, the dependency of Europe on carbon removal technologies increases.\"\n",
    "d5 = \"Changes in lifestyles are crucial, contributing to achieving climate targets sooner.\"\n",
    "corpus = [d1, d2, d3, d4, d5]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary for our corpus. First, we apply a simple preprocessing technique to convert each each sentence into a list of tokens (words). We are using the [Gensim](https://pypi.org/project/gensim/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "processed_corpus = [simple_preprocess(d) for d in corpus]\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our dictionary, obtaining a mapping between each word in our corpus and an associated integer identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHO4IVeaN03k",
    "outputId": "eef74bbd-ecce-4bc6-f867-c53c6c87ec09"
   },
   "outputs": [],
   "source": [
    "# A dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3v9-RrzN0rd",
    "outputId": "d5a44cda-99ce-48d9-baba-8e1bed551aa3"
   },
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHvrJj5yT8uQ"
   },
   "source": [
    "### Bag of Words (BOW)\n",
    "\n",
    "**Bag of Words (BOW)** is the simplest approach to achieve the transformation of documents into vectors. It is divided into two basic steps:\n",
    "- Create a dictionary of unique words from the corpus.\n",
    "- Analyse the documents, i.e. for each word in the dictionary and each document, add 1 if the word is in the document, otherwise 0.\n",
    "\n",
    "Let's implement BOW from scratch using the [spaCy](https://spacy.io/) library. We first define two functions: the first one to get the words of a document and the second to get the unique words of a corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens in document\n",
    "def get_tokens(document):\n",
    "    doc_tokens = ([token.lower_ for token in sp(document) \n",
    "                   if (token.is_punct == False) and (token.is_space == False)])\n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tokens(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkeTK3afT0yz",
    "outputId": "f7e74945-27c0-4c42-fc02-b2b75c3bf7e0"
   },
   "outputs": [],
   "source": [
    "# List of unique words in corpus (dictionary)\n",
    "def vocabulary(corpus):\n",
    "    # Delare output\n",
    "    word_list = []\n",
    "    # Loop documents - lower each word and add it to the output\n",
    "    for document in corpus:\n",
    "        spacy_doc = sp(document)\n",
    "        for token in spacy_doc:\n",
    "            if token.lower_ not in word_list and (token.is_punct == False) and (token.is_space == False):\n",
    "                word_list.append(token.lower_)\n",
    "    # Return output\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFNFfyN6UkAh"
   },
   "source": [
    "We use our two functions to create the Bag of Words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1DSadtqT1ri",
    "outputId": "873193df-a6c4-4b8f-d2ef-3cd0bd84557b"
   },
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "def bow(document, corpus):\n",
    "    # Get tokens\n",
    "    doc_tokens = get_tokens(document)\n",
    "    corpus_tokens = vocabulary(corpus)\n",
    "    # Initialization\n",
    "    bag = {}\n",
    "    for token in corpus_tokens:\n",
    "        bag[token] = 0\n",
    "    # Add 1 if token is in document\n",
    "    for token in doc_tokens:\n",
    "        bag[token] += 1\n",
    "    # Return\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we obtain for the first sentence of our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow(d1, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for all our documents and visualize the result in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "EbfVjwmYYU0H",
    "outputId": "d5af7b0e-720c-4b8c-9cf7-e4c40b8a1e07"
   },
   "outputs": [],
   "source": [
    "# BOW for all documents in corpus\n",
    "bag_of_words = [bow(d, corpus) for d in corpus]\n",
    "\n",
    "# Visualize in dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.DataFrame(bag_of_words,\n",
    "    index= ['d1', 'd2', 'd3', 'd4', 'd5']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr1EQmIeYSqi"
   },
   "source": [
    "Note that this is not perfect. We could (should) remove stopwords, use lemmatization, and potentially consider n-grams.\n",
    "\n",
    "Instead of implementing the technique from scratch, we can rely on the `CountVectorizer` class of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTCKo-uohh_X",
    "outputId": "1eca31e5-c5cc-40d2-bd84-01e0ca972a28"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Learn the vocabulary dictionary and return document-term matrix\n",
    "bag_of_words = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "# DataFrame\n",
    "bag_of_words = pd.DataFrame(bag_of_words, \n",
    "                            columns=vectorizer.get_feature_names_out(),\n",
    "                            index = ['d1', 'd2', 'd3', 'd4', 'd5'])\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoU9xUZLkAZu"
   },
   "source": [
    "Advantages of BOW:\n",
    "- No need of huge corpus of words to get good results in practice.\n",
    "- Easy to understand (i.e., not mathematically complex).\n",
    "\n",
    "Disadvantages of BOW:\n",
    "- A lot of zeros (imagine a corpus of 1000 articles) --> consume memory and space.\n",
    "- Does not maintain any context information (\"I eat a fish\" vs. \"A fish eats me\").\n",
    "- Half solutions: n-grams, specifiying min_df and max_df (see [documentation](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CkpoOenUv7J"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "**TF-IDF** is a type of bag of words approach where instead of adding zeros and ones in the embedding vector, we add floating numbers that contain more useful information. The idea is to emphasize words that appear in few documents in the corpus. A word that appear many times but only in one document will have a high value (close to one) compared to words that appear many times in many documents. This word is then very useful to identify the document.\n",
    "\n",
    "TF-IDF is the product of term frequency (TF) and inverse document frequency (IDF):\n",
    "- **Term Frequency** identifies tokens that appear frequently in a document: \n",
    "    - TF(token, document) = number of times token appears in document / total number of tokens in document\n",
    "    - greater if word appears many times in document\n",
    "- **Inverse Document Frequency** identifies words that appear rarely in the corpus: \n",
    "    - IDF(token, corpus) = log( total number of documents in corpus / number of documents containing token )\n",
    "    - greater if word appears in fewer doucuments\n",
    "    \n",
    "Ok, let's try implement from scratch TF-IDF! First, we define a function to compute the term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsTBjUmWUywM",
    "outputId": "cde8969c-72fb-46a4-fa52-623b0887ad35"
   },
   "outputs": [],
   "source": [
    "# Term frequency (TF)\n",
    "def tf(document):\n",
    "    # Get tokens\n",
    "    tokens = get_tokens(document)\n",
    "    # Initialization \n",
    "    term_freq = {token: 0 for token in tokens}  # Notice the use of comprehension!\n",
    "    # Increment\n",
    "    for token in tokens:\n",
    "        term_freq[token] += 1/len(tokens)\n",
    "    # Return\n",
    "    return term_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each words appear once in our sentence, and the sentence contains 8 words. Hence, the frequency of each token is 1/8=0.125.\n",
    "\n",
    "Let's proceed, defining a function to compute the inverse document frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3h8kaUxrmn5",
    "outputId": "ff3f6e09-c36a-45f3-a66b-fff46376bbb9"
   },
   "outputs": [],
   "source": [
    "# Inverse document frequency\n",
    "def idf(corpus):\n",
    "    # Get list of unique words in corpus\n",
    "    voc = vocabulary(corpus)\n",
    "    # Initialization\n",
    "    inv_doc_freq = {word: 0 for word in voc}\n",
    "    # Number of apparition of word\n",
    "    for word in voc:\n",
    "        for document in corpus:\n",
    "            doc_tokens = get_tokens(document)\n",
    "            if word in doc_tokens:\n",
    "                inv_doc_freq[word] += 1\n",
    "    # IDF\n",
    "    inv_doc_freq = {k: math.log(len(corpus) / inv_doc_freq[k]) for k in inv_doc_freq.keys()}\n",
    "    # Return\n",
    "    return inv_doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, finally we compute TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfgMl15xwOeP",
    "outputId": "255c356b-2beb-4f7e-fe33-bb6759eb5960"
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def tfidf(document, corpus):\n",
    "    # TF\n",
    "    tf_bag = tf(document)\n",
    "    # IDF\n",
    "    idf_bag = idf(corpus)\n",
    "    # TF*IDF\n",
    "    tfidf_bag = {k: tf_bag[k]*idf_bag[k] for k in tf_bag.keys()}\n",
    "    return tfidf_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result for the first sentence of our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf(d1, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to what we obtained with TF, the tokens are now scaled by the IDF. For instance, since \"the\" is a common word, its TF-IDF is lower than words like \"sectors\" that appears only once in our corpus.\n",
    "\n",
    "Let's compute TF-IDF for all tokens and documents and visualize the result in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for all documents\n",
    "bag_of_words_tfidf = [tfidf(doc, corpus) for doc in corpus]\n",
    "\n",
    "# Visualize in Dataframe\n",
    "pd.DataFrame(bag_of_words_tfidf,\n",
    "    index= ['d1', 'd2', 'd3', 'd4', 'd5']\n",
    "    ).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj3x4fCLyHcA"
   },
   "source": [
    "As before with BOW, the result is not perfect since we could remove stopwords, and use n-gramms and lemmas.\n",
    "\n",
    "Instead of implementing the technique from scracth, we can use the `TfidfVectorizer` class of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)). Note that the results differ from ours because `TfidfVectorizer` is using a slightly different formula to compute IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "IZGWejXoyG3r",
    "outputId": "ee83e51b-ca3c-4486-fe17-4687be522715"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Learn the vocabulary dictionary and return document-term matrix\n",
    "bag_of_words_tfidf = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "# DataFrame\n",
    "bag_of_words_tfidf = pd.DataFrame(bag_of_words_tfidf, \n",
    "                                  columns=vectorizer.get_feature_names_out(),\n",
    "                                  index = ['d1', 'd2', 'd3', 'd4', 'd5'])\n",
    "bag_of_words_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFsl7S2FyxxH"
   },
   "source": [
    "Advantage of TF-IDF:\n",
    "- Smart way of representing documents in corpus. More information is provided.\n",
    "\n",
    "Disadvantages of TF-IDF (same as for BOW):\n",
    "- A lot of zeros (imagine a corpus of 1000 articles) --> consume memory and space\n",
    "- Does not maintain any context information (\"I eat a fish\" vs. \"A fish eats me\")\n",
    "- Half solutions: n-grams, specifiying min_df and max_df (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkm6F65kUzR_"
   },
   "source": [
    "## Introduction to Gensim and Word Embedding\n",
    "\n",
    "With BOW and TF-IDF, similar sentences/words have a completely different representation. Thus, sentences with different words but same meaning/semantics will be very distant.\n",
    "\n",
    "In the following, we illustrate how we can find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering.\n",
    "\n",
    "We will use the [Gensim](https://pypi.org/project/gensim/) library. Gensim stands for \"Generate Similar\". It is a popular open-source natural language processing (NLP) library used for unsupervised topic modeling. A complete tutorial can be found [here](https://www.tutorialspoint.com/gensim/gensim_introduction.htm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-lFcpJJPXAq"
   },
   "source": [
    "### Background\n",
    "\n",
    "Word embedding approaches use deep learning and neural network-based techniques to convert words into corresponding vectors so that semantically similar vectors are close to each other in an N-dimensional space, where N refers to the dimensions of the vectors. The underlying assumption is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model.\n",
    "\n",
    "Two word embedding methods:\n",
    "- [Word2vec](https://en.wikipedia.org/wiki/Word2vec), by Google\n",
    "- [GloVe](https://en.wikipedia.org/wiki/GloVe) (Global vectors for Word Representation), by Stanford\n",
    "\n",
    "Word2vec gives astonishing results. Its ability to maintain a semantic relationship is reflected in a classic example where if you have a vector for the word \"King\" and you remove the vector represented by the word \"Man\" from the \"King\" and add \"Woman\", you get a vector that is close to the vector \"Queen\": \n",
    "- King - Man + Woman = Queen\n",
    "\n",
    "Second example: \"dog\", \"puppy\" and \"pup\" are often used in similar situations, with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and according to Word2vec they will therefore share a similar vector representation.\n",
    "\n",
    "In real applications, Word2vec models are created from billions of documents. For example, [Google's Word2Vec model](https://code.google.com/archive/p/word2vec/) is formed from 3 million words and phrases.\n",
    "\n",
    "GloVe is an extension of Word2vec. More information [here](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Recently, more advanced models have been developed, such as [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) - Bidirectional Encoder Representations from Transformers-  and [GPT-3](https://en.wikipedia.org/wiki/GPT-3) - Generative Pre-trained Transformer 3. While Word2vec models represent tokens (word) with a single vector, BERT generates different output vectors for a same word when used in different context. You can find further readings on the topic at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Word2vec with Gensim\n",
    "\n",
    "We will implement Word2vec using the Gensim library. We are going to use a corpus of text extracted from Wikipedia by web scrapping. We first define a function to retrieve texts from a Wikipedia url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4ONCENA5RHYX",
    "outputId": "66edf18a-3dba-4b13-a1fa-aafce604fb49"
   },
   "outputs": [],
   "source": [
    "# Get texts from Wikipedia\n",
    "def get_text(url):\n",
    "    # Retrieve data\n",
    "    scrapped_data = urllib.request.urlopen(url)\n",
    "    article = scrapped_data.read()\n",
    "    # Parse data: # The text is contained in the HTML tag 'p'\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "    paragraphs = parsed_article.find_all('p')  \n",
    "    # Create a string with all the paragraphs\n",
    "    article_text = \"\"\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the Wikipedia articles on [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) and on [Artificial Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence). This will be our corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles\n",
    "machine_learning = get_text(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
    "ai = get_text(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
    "\n",
    "print(machine_learning[:705])\n",
    "print(ai[:741])\n",
    "\n",
    "# Group texts in list\n",
    "texts = [machine_learning, ai]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we preprocess out texts. We create a tokenizer function to lemmatize each token and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56Z0fduhRMb5"
   },
   "outputs": [],
   "source": [
    "# Create tokenizer function for preprocessing\n",
    "def spacy_tokenizer(text):\n",
    "\n",
    "    # Define stopwords, punctuation, and numbers\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    punctuations = string.punctuation +'–' + '—'\n",
    "    numbers = \"0123456789\"\n",
    "\n",
    "    # Create spacy object\n",
    "    mytokens = sp(text)\n",
    "\n",
    "    # Lemmatize each token and convert each token into lowercase\n",
    "    mytokens = ([ word.lemma_.lower().strip() for word in mytokens ])\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    mytokens = ([ word for word in mytokens \n",
    "                 if word not in stop_words and word not in punctuations ])\n",
    "\n",
    "    # Remove sufix like \".[1\" in \"experience.[1\"\n",
    "    mytokens_2 = []\n",
    "    for word in mytokens:\n",
    "        for char in word:\n",
    "            if (char in punctuations) or (char in numbers):\n",
    "                word = word.replace(char, \"\")\n",
    "        if word != \"\":\n",
    "            mytokens_2.append(word)\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our function to tokenize our corpus of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "processed_texts = [spacy_tokenizer(text) for text in texts]\n",
    "\n",
    "for processed_text in processed_texts:\n",
    "    print(processed_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is preprocessed, we can train a Word2vec model. We use the `Word2Vec` module of Gensim ([Documentation](https://radimrehurek.com/gensim/models/word2vec.html)). As input, we provide the processed texts, i.e., a list of lists of tokens. In addition, we use as parameters:\n",
    "- `min_count`: minimum number of occurence of single word in corpus to be taken into account\n",
    "- `vector_size`: dimension of the vectors representing the tokens\n",
    "\n",
    "Once the model is trained, we can access to the mapping between words and embeddings with the method `.wv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYbGv4LbRRlZ",
    "outputId": "c3316ba2-e5d6-4226-8d53-8343062eb425"
   },
   "outputs": [],
   "source": [
    "# Word embedding \n",
    "word2vec = Word2Vec(processed_texts, min_count=2, vector_size=100)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = word2vec.wv.key_to_index\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token (word) is represented by a vector (array) of size 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynLxnGryTXKc",
    "outputId": "403fd91d-f8c1-4d37-e849-075110903009"
   },
   "outputs": [],
   "source": [
    "# Vector\n",
    "v1 = word2vec.wv['intelligence'] \n",
    "v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this space, we can explore the similarities between tokens. For instance, let's find the most similar words to \"intelligence\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCQRhSF5bPFI",
    "outputId": "a5e38f27-28bb-45e5-9a49-ec33bb865d03"
   },
   "outputs": [],
   "source": [
    "# Similar vectors/words\n",
    "sim_words = word2vec.wv.most_similar('intelligence')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the similarity between two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRBwWMjtcrOH",
    "outputId": "7d14ad52-266b-45d3-e54b-a2e66dcd857d"
   },
   "outputs": [],
   "source": [
    "# Similarity between two words\n",
    "print('The similarity between \"computer\" and \"animal\" is: ', word2vec.wv.similarity('computer', 'animal'))\n",
    "print('The similarity between \"computer\" and \"machine\" is: ', word2vec.wv.similarity('computer', 'machine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwoQg0xEf9kN"
   },
   "source": [
    "Remarks:\n",
    "- There are other models than Word2Vec in Gensim. For instance, `Doc2Vec` is used to create a vectorised representation of a group of words (i.e., a document) taken collectively as a single unit (illustrated in the next section).\n",
    "- Gensim has many applications besides word embedding, see e.g., [topic modelling](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/). Feel free to explore the library!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-_83p20NjO-"
   },
   "source": [
    "### Your turn\n",
    "\n",
    "- Using the functions defined above, create a corpus of documents with the following Wikipedia articles: [Photovoltaics](https://en.wikipedia.org/wiki/Photovoltaics), [Wind turbine](https://en.wikipedia.org/wiki/Wind_turbine), [Hydropower](https://en.wikipedia.org/wiki/Hydropower), and [Nuclear power plant](https://en.wikipedia.org/wiki/Nuclear_power_plant). Do you know the share of each technology in the Swiss electricity mix? Check the [Electricity sector in Switzerland](https://en.wikipedia.org/wiki/Electricity_sector_in_Switzerland) for the answer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing: Tokenize your corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTiVRKpjcrJT"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the number of occurrences of the word \"energy\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz5BBzckeLAe"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Word2Vec representation of the article with a min_count of 1 and a vector size of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-1RbfX7fpzO"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the 10 most similar words to \"electricity\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3he4niYadil"
   },
   "source": [
    "## Application: Text Classification with TF-IDF vs Doc2Vec\n",
    "\n",
    "In this section, we do an application on text classification to illustrate how the embedding can influence the accuracy of a classifier. \n",
    "\n",
    "Our goal is to classify consumer finance complaints into 12 pre-defined categories using:\n",
    "- TF-IDF and logistic regression\n",
    "- Doc2Vec and logistic regression\n",
    "We use the same tokenizer function, train-test split, classification algorithm, etc. The only difference is the mathematical representation (i.e., the vectorization from the tokens) of the complaints.\n",
    "\n",
    "This application was inspired by the articles published by Susan Li on Towards Data Science:\n",
    "- [Multi-Class Text Classification with Scikit-Learn](https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f)\n",
    "- [Multi-Class Text Classification with Doc2Vec & Logistic Regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean data\n",
    "\n",
    "We work with a sample of a large dataset from Data.gov that can be found [here](https://catalog.data.gov/dataset/consumer-complaint-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "xiRTQshsihUu",
    "outputId": "0a96f12f-5610-4990-92f3-953668a53acf"
   },
   "outputs": [],
   "source": [
    "# Load data from GitHub\n",
    "path = \"https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/complaints_sample.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cys9L2XidZsq",
    "outputId": "e627051e-e3e0-4356-c377-ac30ee0be747"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pyg-rztyPJ6e"
   },
   "source": [
    "The data set includes 18 columns and 9101 rows describing consumer complaints about financial products. In this case, we want to predict the `Product` category based on the text of the complaint (i.e., `Consumer complaint narrative`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9mgx17mnpy2s",
    "outputId": "e1e1c9c5-7db2-4fac-f025-667c6888360b"
   },
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "data = df[[\"Product\", \"Consumer complaint narrative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhB01GG6QXVP"
   },
   "source": [
    "Around 2/3 of the complaints are null values. They are not useful for the prediction so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "mbno4wghqCNH",
    "outputId": "f8d6d432-5289-41cb-beb7-a62502ca55fe"
   },
   "outputs": [],
   "source": [
    "# Drop NaN\n",
    "print(data.isnull().sum())\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgb3yc3KqCLr",
    "outputId": "44ce8643-a925-47a7-b49c-72392f507aff"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCasthTDQrYb"
   },
   "source": [
    "We end up with 3137 complaints for which we would like to predict the product concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc8z_6SkTIx2"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start by an EDA to better understand our data and inform our analysis. First note that we are dealing with a dataset containing a large number of words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1JQ8GFf0ihb3",
    "outputId": "41aabd76-265b-4723-9a9d-56a0e0ffbe40"
   },
   "outputs": [],
   "source": [
    "# Total number of words - over 600,000\n",
    "words_number = data['Consumer complaint narrative'].apply(lambda x: len(x.split(' '))).sum()\n",
    "print(f'The complaints contain {words_number} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract a sample to see how the complaints look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "ppQTFaZ6t9dy",
    "outputId": "a4f1eab2-5572-4fee-a9f7-4191bfa7b27c"
   },
   "outputs": [],
   "source": [
    "# Sample\n",
    "data['Consumer complaint narrative'].sample().values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDpSTJyZQ6O5"
   },
   "source": [
    "The data has been anonymized (i.e., names, dates, IDs, etc. have been replaced by XXXX).\n",
    "\n",
    "Next, note that the classes (products) are imbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdDpzPPouaSN",
    "outputId": "12f3b81f-1785-40db-cd70-9d4265916d07"
   },
   "outputs": [],
   "source": [
    "# Imbalanced dataset\n",
    "data.Product.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDon6qpmRMzo"
   },
   "source": [
    "There are 17 categories. We group some of them together (e.g. \"Credit card\", \"Prepaid card\", and \"Credit or prepaid card\") because they are sub-categories of each other. We end up with 12 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "er1kzzv3vBSt",
    "outputId": "ca3d64ce-7599-469d-8586-99bd4bf85b7e"
   },
   "outputs": [],
   "source": [
    "# Merge categories\n",
    "dic_replace = {'Credit reporting':'Credit reporting, credit repair services, or other personal consumer reports', \n",
    "               'Credit card':'Credit card or prepaid card', \n",
    "               'Payday loan':'Payday loan, title loan, or personal loan', \n",
    "               'Money transfers':'Money transfer, virtual currency, or money service',\n",
    "               'Prepaid card':'Credit card or prepaid card',\n",
    "               'Virtual currency':'Money transfer, virtual currency, or money service'}\n",
    "data.replace(dic_replace, inplace=True)\n",
    "data.Product.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the number of observation per product using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "id": "iEeOpaMQtgAA",
    "outputId": "3f261e91-8d48-42e9-ee77-469d1f318367"
   },
   "outputs": [],
   "source": [
    "# Plot number of complaints per category\n",
    "cnt_pro = data['Product'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(x=data['Product'], order = cnt_pro.index)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Product', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the base rate, i.e., the accuracy obtained using a naive classifier that predicts that all observations are from the largest class (\"Credit reporting, credit repair services, or other personal consumer reports\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JdF1razudYJ",
    "outputId": "669f604d-9a0c-4059-e06e-cec3f7e2f4d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base rate\n",
    "base_rate = round(len(data[data.Product == \"Credit reporting, credit repair services, or other personal consumer reports\"]) / len (data), 4)\n",
    "print(f'The base rate is: {base_rate*100:0.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBr6OSytTLgD"
   },
   "source": [
    "### Classification using TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our training and test set, using the `train_test_split` module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "X = data['Consumer complaint narrative'] # Features we want to analyze\n",
    "ylabels = data['Product']                # Labels we test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `TfidfVectorizer` class of sklearn for the word embedding. Since we are dealing with very specific data (e.g., the anonymization process generated non-standard sequence of characters), we are defining our own tokenizer function, which we can use as parameter of `TfidfVectorizer` instead of the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXHnUbjerLPD"
   },
   "outputs": [],
   "source": [
    "# Define tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "\n",
    "    punctuations = string.punctuation\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    # Create token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = sp(sentence)\n",
    "\n",
    "    # Lemmatize each token and convert each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # Remove anonymous dates and people\n",
    "    mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"] ]\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As other parameters of `TfidfVectorizer`, we are using token and pair of tokens (`ngram_range = (1,2)`) and we ignore terms that have a document frequency strictly lower than 5 (`min_df = 5`). \n",
    "\n",
    "Note that we also rely on the `Pipeline` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)) to sequentially apply models, first the vectorizer, then the classifier. We also time our training (it might take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tTbx3oswIan",
    "outputId": "276c2c47-d4da-40f7-83f1-04939c7c0da0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define vectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
    "\n",
    "# Define classifier\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we predict the test set values and evalute the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 895
    },
    "id": "dC3Pbqxlxod2",
    "outputId": "f6c43488-56a2-4ccc-d371-b5f6616eec65"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "## Accuracy\n",
    "accuracy_tfidf = round(accuracy_score(y_test, y_pred), 4)\n",
    "print(f'The accuracy using TF-IDF is: {accuracy_tfidf*100:0.2f}%')\n",
    "\n",
    "## Confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Thl75Ei5TRZz"
   },
   "source": [
    "### Classification using Doc2Vec and Logistic Regression\n",
    "\n",
    "We now try to do the same exercise, but using [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html). While Word2Vec computes a feature vector for every word in the corpus, Doc2Vec computes a feature vector for every document (i.e., sentence) in the corpus.\n",
    "\n",
    "We first tokenize our data, using the same tokenizer as before. We use `TaggedDocument` from Gensim to obtain the appropriate input document format for `Doc2Vec`. `TaggedDocument` returns, for each observation, a document containing *words* (i.e., a list of tokens) and the associated *tags* (our label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDBzqEryVgSY",
    "outputId": "99282cb7-2281-45bf-c4e2-130715a1a095"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize data - same tokenizer function as before\n",
    "sample_tagged = data.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['Consumer complaint narrative']), tags=[r.Product]), axis=1)\n",
    "print(sample_tagged.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hatOMpSPAqw",
    "outputId": "2dc05c09-fbc6-455f-9286-c61cfdc8e8c5"
   },
   "outputs": [],
   "source": [
    "sample_tagged.values[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our dataset into training and test, using the same split as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdSvlQf26uZb",
    "outputId": "49e6984c-bec4-4c8b-c6e3-469f407c2fa7"
   },
   "outputs": [],
   "source": [
    "# Train test split - same split as before\n",
    "train_tagged, test_tagged = train_test_split(sample_tagged, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the training process, we use the `multiprocessing` library ([Documentation](https://docs.python.org/3/library/multiprocessing.html)). It will allow to train the model using several worker threads via the parameter `workers` of `Doc2Vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY_KwtL8ThRV"
   },
   "outputs": [],
   "source": [
    "# Number of CPUs in the system\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the `Doc2Vec` model, using as parameters:\n",
    "- `dm`: training algorithm, if 1 distributed memory is used (PV-DM), else distributed bag of words (PV-DBOW)\n",
    "- `vector_size`: dimension of feature vector\n",
    "- `hs`: if 1, hierarchical softmax will be used for model training; if set to 0, and negative is non-zero, negative sampling will be used\n",
    "- `negative`: specifies how many \"noise words\" should be drawn for negative sampling\n",
    "- `min_count`: ignore words with frequency lower than this\n",
    "- `epochs`: number of iteration (epoch) over the corpus\n",
    "\n",
    "In addition, we build our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhkoBMP_o0HN",
    "outputId": "b918b893-abb4-4116-eb35-1eef2a3c3b40"
   },
   "outputs": [],
   "source": [
    "# Define Doc2Vec and build vocabulary\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epochs=300)\n",
    "model_dbow.build_vocab([x for x in train_tagged.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqBwLmHGU8XD"
   },
   "source": [
    "We now train the distributed bag of words model. In short, it trains a neural network and the optimal weights are the coefficients of the vectors of the documents. Therefore, similar documents will be close to each other in the N-dimentional space (N being the size of the vectors). More information on this [here](https://thinkinfi.com/simple-doc2vec-explained/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDU2E5fCpHVj"
   },
   "outputs": [],
   "source": [
    "# Train distributed Bag of Word model\n",
    "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our tagged documents into vectors using the model we trained. We define a function that use as input our model and tagged documents and returns the targets (i.e., labels) and regressors, i.e., the vector representation of the complaints. We then apply this function to prepare the training and test set for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hq_UN6_U0hF"
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, epochs=100)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each document (i.e., complaint) is now a vector in the space of 30 dimentions. Similar complaints should have similar vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O98HjFkWs6QI",
    "outputId": "13e9b1b5-d7eb-4849-fee6-05d07f8fb079"
   },
   "outputs": [],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we can finally implement our logistic regression. We proceed as before, training the model, predicting on the test set, and evaluating the performance of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "j0fd2XSFVz1r",
    "outputId": "20b37cc6-b08d-48f0-c0f7-4d97e9f670f1"
   },
   "outputs": [],
   "source": [
    "# Fit model on training set - same algorithm as before\n",
    "logreg = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "## Accuracy\n",
    "accuracy_doc2vec = round(accuracy_score(y_test, y_pred), 4)\n",
    "print(f'The accuracy using TF-IDF is: {accuracy_doc2vec*100:0.2f}%')\n",
    "\n",
    "## Confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve the accuracy of a text classifier?\n",
    "\n",
    "In order to improve the prediction, we can try to:\n",
    "- Resample our data (i.e., create balanced dataset)\n",
    "- Tune the hyperparameters of the model\n",
    "- Improve text preparation\n",
    "- Use another classifier, e.g., k-NN, decision trees, random forests, etc.\n",
    "- All of the above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_jHZwiHjnu_"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "Text Analytics is a rich field. We have seen above one application of text classification. A common application of text classification is [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis), which is the process of tagging data according to their sentiment, such as positive, negative and neutral. You can find a guide to sentiment analysis [here](https://huggingface.co/blog/sentiment-analysis-python). You can also find many already trained models on [Huggingface](https://huggingface.co/models), including ready-made classifier for sentiment analysis.\n",
    "\n",
    "Finally, here are some resources on Word2Vec, GloVe, BERT to deepen your understanding of the topic:\n",
    "- Rong, X. (2014). word2vec parameter learning explained. *arXiv preprint arXiv:*[1411.2738](https://arxiv.org/abs/1411.2738)\n",
    "- [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "- [Word Embeddings for NLP](https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4)\n",
    "- [NLP — Word Embedding & GloVe](https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6)\n",
    "- [Intuitive Guide to Understanding GloVe Embeddings](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)\n",
    "- [Word2vec vs BERT](https://medium.com/@ankiit/word2vec-vs-bert-d04ab3ade4c9)\n",
    "- [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "r-lFcpJJPXAq",
    "z-_83p20NjO-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
