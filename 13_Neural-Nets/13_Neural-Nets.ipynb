{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7497e1-2824-4a50-b0d9-98d71ce60081",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/edoardochiarotti/class_datascience/blob/main/2023/03_EDA-Visualization/Resources/03_EDA_Data-visualization.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# ML import\n",
    "from transformers import pipeline              # Pre-trained models\n",
    "from sklearn.model_selection import train_test_split # Splitting the data set\n",
    "from sklearn.preprocessing import MinMaxScaler       # Normalization\n",
    "import torch                                   # PyTorch\n",
    "import torch.nn as nn                          # PyTorch building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984e553-36af-42c8-9522-dd090686c913",
   "metadata": {},
   "source": [
    "<img src='https://imgs.xkcd.com/comics/trained_a_neural_net.png' width=\"270\">\n",
    "\n",
    "Source: [xqcd 2173](https://xkcd.com/2173/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on Neural Nets. After presenting the main concepts, you will be introduced to the techniques to implement your own neural network in Python, with PyTorch and using a pre-build Neural Net. \n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [Background](#Background)\n",
    "    - [Neurons](#Neurons)\n",
    "        - [Biological Neuron](#Biological-Neuron)\n",
    "        - [Artificial neuron and perceptron](#Artificial-neuron-and-perceptron)\n",
    "            - [Activation function](#Activation-function)\n",
    "    - [Some Neural Nets](#Some-Neural-Nets)\n",
    "        - [Multilayer Perceptron (MLP)](#Multilayer-Perceptron-(MLP))\n",
    "        - [Convolutional Neural Nerwork (CNN)](#Convolutional-Neural-Nerwork-(CNN))\n",
    "        - [Recurrent Neural Network (RNN)](#Recurrent-Neural-Network-(RNN))\n",
    "- [How to build your own Neural Net?](#How-to-build-your-own-Neural-Net?)\n",
    "    - [Application: Predicting house prices](#Application:-Predicting-house-prices)\n",
    "        - [Data](#Data)\n",
    "            - [Preprocessing](#Preprocessing)\n",
    "            - [Creating training and test set](#Creating-training-and-test-set)\n",
    "            - [Normalizing-the-data](#Normalizing-the-data)\n",
    "        - [Building a Linear Regression model with PyTorch](#Building-a-Linear-Regression-model-with-PyTorch)\n",
    "            - [Create tensors](#Create-tensors)\n",
    "            - [Define and train a model with PyTorch](#Define-and-train-a-model-with-PyTorch)\n",
    "        - [Your turn](#Your-turn)\n",
    "- [Implement a pre-built Neural Net](#Implement-a-pre-built-Neural-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763984b-c9d6-43b0-ba06-fd749431aa32",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) - simply called Neural Nets (NNs) - are computing systems inspired by the biological neural networks that constitute animal brains. They are used to approximate functions that are generally unknown.\n",
    "\n",
    "NNs are based on a collection of connected nodes, called **artificial neurons**. In short, an [artificial neuron](https://en.wikipedia.org/wiki/Artificial_neuron) is a mathematical function conceived as a model of biological neurons. \n",
    "\n",
    "These artificial neurons are aggregated into **layers**. Signals travel from the first layer (the *input layer*), to the last layer (the *output layer*), traversing one or several *hidden layers*, which perform different transformations on their inputs. The output of one layer is the input of the next one: this is called **forward propagation**. A neural network with multiple layers between the input and output layers is called [Deep Neural Network](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks) (DNN).\n",
    "\n",
    "Below is an illustration of a simple NN. Each circle represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another:\n",
    "\n",
    "<center>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/444px-Colored_neural_network.svg.png?20130228185515' width=\"270\"></center>\n",
    "\n",
    "Source: [Glosser.ca](https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg), Wikipedia, [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fa865-07da-4109-b61b-cd4d155fb33d",
   "metadata": {},
   "source": [
    "Depending on the structure of the network, we differentiate between feedforward and recurrent NNs:\n",
    "- [Feedforward Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network) (FFNN)  was the first and simplest type of artificial neural network devised. In FFNNs, connections between the nodes do not form a cycle. FFNN are trained by backward propagation ([**backpropagation**](https://en.wikipedia.org/wiki/Backpropagation)).\n",
    "- [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) are networks wherein connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. RNNs are trained by [**backpropagation through time**](https://en.wikipedia.org/wiki/Backpropagation_through_time) (BPTT).\n",
    "\n",
    "In the following, we will explore the concept of (artificial) neurons and present a few types of neural nets.\n",
    "\n",
    "For a nice visual introduction to Neural Nets and to backpropagation, you can view the series of videos by [3Blue1Brown](https://www.youtube.com/@3blue1brown) on the topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1828da-c74c-44b3-a0a3-352c02ff9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"aircAruvnKk\", width=\"560\", height=\"315\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2f794-7e6b-4438-968d-87cf15802f91",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), by Aurélien Géron\n",
    "- [Everything you need to know about Neural Networks and Backpropagation — Machine Learning Easy and Fun](https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a), by Gavril Ognjanovski\n",
    "- [Neural Network from scratch in Python](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65), by Omar Aflak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74b223-5b65-4bb3-b6a9-4ec89d491649",
   "metadata": {},
   "source": [
    "### Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b7d97-3e8c-4ffc-bc2e-67116d23a889",
   "metadata": {},
   "source": [
    "#### Biological Neuron\n",
    "\n",
    "The nervous system is composed of more than 100 billion cells known as [neurons](https://en.wikipedia.org/wiki/Neuron), which process and transmit the information received from our senses. Neurons are arranged together in our brain to form a network of nerves. These nerves pass electrical impulses from one neuron to the other.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vBIWWCFLLzZzGes1HcCJOw.jpeg' width=\"450\">\n",
    "\n",
    "Neurons are made up of three major parts: \n",
    "- the *dendrites* collect information from other cells and send this information to the soma;\n",
    "- the *soma* is the cell body containing the nucleus of the cell and keeping it alive;\n",
    "- the *axon* transmits information away from the cell body towards other neurons or to the muscles and glands;\n",
    "    - when the axon is stimulated by an electrical signal from the dendrites, the impulse is transmitted if the electrical signal is strong enough that it passes a certain level or threshold.\n",
    "    - the axon terminal, located at the end of the axon farthest from the soma, contains [*synapses*](https://en.wikipedia.org/wiki/Synapse), which are structure that permits the transmission of information to another cell. Synapses have the ability to strengthen or weaken over time in responses to increases or decreases in their activity: this is called [*synaptic plasticity*](https://en.wikipedia.org/wiki/Synaptic_plasticity).\n",
    "\n",
    "<center>\n",
    "<img src='https://opentextbc.ca/introductiontopsychology/wp-content/uploads/sites/9/2013/11/6a3f0732c22683476ea201ffc5e428ad.jpg' width=\"400\"></center>\n",
    "\n",
    "Source: Jennifer Walinga, [The Neuron Is the Building Block of the Nervous System](https://opentextbc.ca/introductiontopsychology/chapter/3-1-the-neuron-is-the-building-block-of-the-nervous-system/), Chapter 4.1 of *Introduction to Psychology*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe51c9-40ab-4ad4-a54f-19563870533d",
   "metadata": {},
   "source": [
    "#### Artificial neuron and perceptron\n",
    "\n",
    "An [artificial neuron](https://en.wikipedia.org/wiki/Artificial_neuron) is inspired by its biological counterpart. It receives some inputs, transforms them, and transmits the output.  \n",
    "\n",
    "An artificial neuron typically: 1) performs a weighted sum of the inputs; 2) passes this sum through an [activation function](https://en.wikipedia.org/wiki/Activation_function). We'll discuss what is an activation function below. For now, let's formalize what is happening in an artificial neuron. Let: \n",
    "- $\\boldsymbol{x}=(x_1, ..., x_d)$ be some input signals\n",
    "    - the input signals can come from artificial neurons from the input layer (input data) or from artificial neurons from the hidden layers\n",
    "- $y_k$ be the output signal of neuron $k$\n",
    "\n",
    "The operation performed by an artificial neuron is: \n",
    "$$y_k = \\varphi(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = \\varphi(w_{k1} x_{1} +  w_{k2} x_{2} + ... +  w_{kd} x_{d} + b_k) $$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{w}=(w_{k1}, ..., w_{kd})$ are the **weights**\n",
    "    -  the weights have the same role than the synapses. Their value determines the strength of the link between one artificial neuron and another. When we learn the weights, this link can strengthen or weaken, similarly than with synaptic plasticity\n",
    "- $b_k$ is the **bias**\n",
    "- $\\varphi$ is an **activation function**\n",
    "    - the activation function models what is happening in the axon of a biological neuron: the electrical impulses are only transmitted if they are strong enough to reach a given threshold\n",
    "\n",
    "<center>\n",
    "<img src='https://www.gabormelli.com/RKB/images/thumb/3/31/artificial-neuron-model.png/600px-artificial-neuron-model.png' width=\"450\"></center>\n",
    "\n",
    "Source: [Artificial Neuron](https://www.gabormelli.com/RKB/Artificial_Neuron), Gabor Melli's Research Knowledge Base\n",
    "\n",
    "Note that, oftentimes, we write $b_k=w_{k0}$ and $x_{0}=1$, so that the expression simplifies to $y_k = \\varphi(\\boldsymbol{w} \\cdot \\boldsymbol{x})$.\n",
    "\n",
    "The first artificial neuron was the [perceptron](https://en.wikipedia.org/wiki/Perceptron), invented in 1943 by Warren McCulloch and Walter Pitts. It was a binary classifier, i.e., the activation function was the unit step function: \n",
    "- $\\varphi(\\boldsymbol{x}) = 1$ if $\\boldsymbol{w} \\cdot \\boldsymbol{x} +b>0$\n",
    "- $\\varphi(\\boldsymbol{x}) = 0$ otherwise\n",
    "\n",
    "The perceptron, also called the single-layer perceptron, was the first and simplest (feedforward) neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be768579-7a57-43d8-b26b-b6abc9efeb92",
   "metadata": {},
   "source": [
    "##### Activation function\n",
    "\n",
    "As mentioned above, the [activation function](https://en.wikipedia.org/wiki/Activation_function) represents what is happening in the axon of a biological neuron: the electrical impulses are only transmitted if they are strong enough to reach a given threshold. There are many possible activation functions:\n",
    "- unit step function ([Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function)), as used in the perceptron;\n",
    "- sigmoid, e.g., the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), previously seen for logistic regression\n",
    "    - $\\varphi(x)=\\frac{1}{1 + e^{-x}}$\n",
    "- Rectified linear unit ([ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))), one of the most popular because of its good performance and fast gradient computation\n",
    "    - $\\varphi(x) = \\max(0,x) $\n",
    "- Parametric rectified linear unit ([PReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Parametric_ReLU)), a variant of ReLU that avoids the *dead* neurons problem, i.e., neurons becoming inactive\n",
    "    - $\\varphi(x) = \\max(\\alpha x,x) $ with $0<\\alpha<1$\n",
    "    - when $\\alpha=0.01$, the function is called Leaky ReLU\n",
    "\n",
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypsvQH7kvtI2BhzR2eT_Sw.png' width=\"400\"> </center>\n",
    "\n",
    "Source: Danqing Liu, [A Practical Guide to ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "\n",
    "There are numerous other activation functions. Explore them [here](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5654a34-b6ea-4513-968d-a4af75e4efd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Some Neural Nets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca625a0-b1b7-4d5d-8660-ea33f7a22b1e",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron (MLP)\n",
    "\n",
    "A [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) is a *fully-connected* feedforward neural network. Here is an illustration, with 3 hidden layers:\n",
    "\n",
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:569/0*03elfV9p5nZTnRKe.png' width=\"500\"> </center>\n",
    "\n",
    "Each circle represents one perceptron. Historically, the perceptrons used a sigmoid activation function, but nowadays, ReLU and its variants are more frequently used.\n",
    "\n",
    "Originally developed in the 1960s, these models had applications in diverse fields in the 1980s, such as speech recognition and machine translation software. Recently, interest in these networks was renewed due to the successes of deep learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5a330-3cac-42a5-9079-c7b564993c0b",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network (CNN)\n",
    "\n",
    "A [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) is a class of neural network most commonly applied in processing data that has a grid-like topology, such as an image. They have proven very effective in analyzing visual imagery, e.g., image recognition and classification. \n",
    "\n",
    "CNNs use a mathematical operation called [convolution](https://en.wikipedia.org/wiki/Convolution) in place of general matrix multiplication in at least one of their layers. This layer performs a dot product between two matrices, where one matrix is the set of learnable parameters otherwise known as a [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing)), and the other matrix is the restricted portion of the receptive field. In other words, a convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel:\n",
    "\n",
    "<center>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif' width=\"300\"> </center>\n",
    "\n",
    "Source: Michael Plotke, [Kernel (image processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing))\n",
    "\n",
    "The kernel is a filter that applies a transformation to the original image, e.g., blurring, sharpening, embossing, edge detection, etc. \n",
    "\n",
    "For a more detailed introduction of CNN, you can read:\n",
    "- [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/), by Ujjwal Karn\n",
    "- [Convolutional Neural Networks, Explained](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939), by Mayank Mishra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ede07-61c4-42bc-85d5-191bdb70e9af",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network (RNN)\n",
    "\n",
    "A [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. RNNs are useful when dealing with sequential data like natural language and time series. They are mostly used in the fields of natural language processing and speech recognition.\n",
    "\n",
    "A RNN is very much alike feedforward neural nerworks, except it also has connections pointing backforward: at each time step $t$, a *recurrent neuron* receives the input $x_t$ as well as its own output from the previous step $y_{t-1}$:\n",
    "$$y_{t} = \\varphi(\\boldsymbol{w_t} \\cdot \\boldsymbol{x_t} + \\boldsymbol{w_y} \\cdot \\boldsymbol{y_{t-1}} + b)$$\n",
    "\n",
    "We can represent a recurrent neuron graphically:\n",
    "<center>\n",
    "<img src='https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1401.png' width=\"400\"> </center>\n",
    "\n",
    "\n",
    "More generally, a recurrent unit may store more information than the previous predictions. A part of a neural network that preserves some state across time steps is called a *memory cell*. Indeed, since the output at time $t$ is a function of the inputs from previous time steps, the recurrent neuron has a form of memory! The memory cell is a hidden layer that flows through time:\n",
    "\n",
    "<center>\n",
    "<img src='https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1403.png' width=\"400\"> </center>\n",
    "\n",
    "\n",
    "RNNs have the obvious advantage of taking into account historical information. However, computation can be slow and it might be difficult to access information from a long time ago (issue of *fading memory*). To solve this issue, various memory cell architectures with long-term memory have been developed to save information, including:  \n",
    "- The [Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) with the idea to store a short-term state $h_t$ and a long-term state $c_t$. LTSM rely on *gates*: when the gate is open, the output is multiplied by 1 (information preserved); when the gate is closed, the outputs is multiplied by 0 (information erased). Gates are a way to update and reset persistent information:\n",
    "    - the *forget gate* controls which parts of the long-term state should be erased,\n",
    "    - the *input gate* controls which parts of the output should be added to the long-term state,\n",
    "    - the *output gate* controls which parts should be output at this time step\n",
    "- The [Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) (GRU), a simplified version of the LTSM.\n",
    "\n",
    "<center>\n",
    "<img src='https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1413.png' width=\"400\"> </center>\n",
    "\n",
    "For a more detailed introduction of RNN, you can read:\n",
    "- [Source behind all the figures in this section]: [Chapter 4. Recurrent Neural Networks](https://www.oreilly.com/library/view/neural-networks-and/9781492037354/ch04.html), *Neural networks and deep learning*, by Aurélien Géron\n",
    "- [Recurrent Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), by Afshine Amidi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce878d70-296b-4ce2-9a05-a5d0410d79e1",
   "metadata": {},
   "source": [
    "## How to build your own Neural Net?\n",
    "\n",
    "There exists several libraries to implement Neural Nets in Python, the most popular being PyTorch and TensorFlow/Keras: \n",
    "- [PyTorch](https://pytorch.org/) is a deep learning framework based on [Torch](https://en.wikipedia.org/wiki/Torch_(machine_learning)). It was developed by Meta AI (Facebook) and open-sourced in 2017.\n",
    "- [TensorFlow](https://www.tensorflow.org/) is an end-to-end open-source platform for machine learning. It was developed by Google and released as open source in 2015. Its name comes from the basic data structure used, namely [tensors](https://en.wikipedia.org/wiki/Tensor). \n",
    "- [Keras](https://keras.io/) is a high-level neural networks library that is running on the top of TensorFlow, among others. \n",
    "\n",
    "For a comparison of TensorFlow, Keras, and Pytorch, you can refer to:\n",
    "- [PyTorch vs TensorFlow for Your Python Deep Learning Project](https://realpython.com/pytorch-vs-tensorflow/), by Ray Johns\n",
    "- [Keras vs Tensorflow vs Pytorch: Key Differences Among Deep Learning](https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article), by John Terra\n",
    "\n",
    "In this notebook, we are going to implement a Neural Net with PyTorch, displaying the simplicity, ease of use, and flexibility of this library. For a walkthrough on Neural Nets with TensorFlow/Keras, you can for instance refer to [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks), by François Chollet (Chapter 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33d277-2861-493d-8fad-63baec052f8e",
   "metadata": {},
   "source": [
    "### Application: Predicting house prices\n",
    "\n",
    "In this application, we will implement a Linear Regression model with PyTorch to predict house prices using the [Ames Housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439b086-55d0-442f-ab05-0e9399e15659",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c89a5-84f5-4229-a987-1dbeaa878682",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/house_price.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6af7a8-2aad-45ec-af77-7b00a60cfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabbcb8-ef34-46d2-b156-6787824e3a85",
   "metadata": {},
   "source": [
    "The dataset contains 81 columns. A description of the features is available in the file \"[house_price_data_description](https://github.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/blob/main/data/house_price_data_description.txt)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72baf826-a96e-49d7-a6df-57cd349781f3",
   "metadata": {
    "id": "annual-officer"
   },
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd5835-e7e1-451b-8e5e-64968834756a",
   "metadata": {},
   "source": [
    "Let's first extract the features of interest. We will use the numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf3888-4c1a-406f-a199-05145d017ff7",
   "metadata": {
    "id": "identified-wildlife"
   },
   "outputs": [],
   "source": [
    "# Data types\n",
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc199722-8f76-4ad7-9b33-65f76a6bb2bb",
   "metadata": {
    "id": "rubber-corrections"
   },
   "outputs": [],
   "source": [
    "# Display numeric features (integer and floats)\n",
    "numeric_columns = list(raw_data.columns[(raw_data.dtypes==np.int64) |\n",
    "                 (raw_data.dtypes==np.float64)])\n",
    "print(numeric_columns, \"\\n\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9354882-1469-42ee-8fd6-38301f89ed0f",
   "metadata": {
    "id": "announced-incidence"
   },
   "source": [
    "`SalePrice` is the value we want to predict. We set it as the last column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd7159-374a-47b0-ae56-5864dbac3cf8",
   "metadata": {
    "id": "intermediate-cable"
   },
   "outputs": [],
   "source": [
    "# Output SalePrice as last column\n",
    "numeric_columns.remove('SalePrice')\n",
    "numeric_columns.append('SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2153a-be33-48b1-b191-31f95b4734aa",
   "metadata": {
    "id": "extended-attribute"
   },
   "source": [
    "We also remove the `Id` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e372a0d-4c5c-444d-bd1e-7fd8d411d84c",
   "metadata": {
    "id": "marked-cancellation"
   },
   "outputs": [],
   "source": [
    "# Remove Id\n",
    "numeric_columns.remove('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c160a2-ec7f-4690-9eba-464cfffe2e7a",
   "metadata": {
    "id": "returning-vertex"
   },
   "source": [
    "Now we extract the numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410bfa5-0140-4d42-a766-c3a553d67683",
   "metadata": {
    "id": "irish-probe"
   },
   "outputs": [],
   "source": [
    "# Extract numeric data\n",
    "numeric_data = raw_data[numeric_columns]\n",
    "numeric_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ca5a4-e780-4b42-a304-01c5b6a915b2",
   "metadata": {
    "id": "warming-cartoon"
   },
   "source": [
    "Now let's deal with the missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9084d6e-2a1f-4e81-ab29-a575cf340478",
   "metadata": {
    "id": "obvious-rider"
   },
   "outputs": [],
   "source": [
    "# Display features with missing values\n",
    "nan_columns = np.any(pd.isna(numeric_data), axis = 0)\n",
    "nan_columns = list(nan_columns[nan_columns == True].index)\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bbb00-d6db-47a8-b295-8652159c69c1",
   "metadata": {
    "id": "activated-center"
   },
   "source": [
    "We simply replace them with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3bbd9-c641-4864-b657-b193d1659f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NAN with 0\n",
    "numeric_data = numeric_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3244053-ee49-4f29-b4c1-aefe674bdb33",
   "metadata": {
    "id": "joined-grade"
   },
   "source": [
    "##### Creating training and test set\n",
    "\n",
    "Let's split the data for training and test. We use the `train_test_split` module of `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91354d50-6616-4454-a8cf-3aa8a615d9e6",
   "metadata": {
    "id": "northern-welsh"
   },
   "outputs": [],
   "source": [
    "# Splitting training/test set\n",
    "numeric_data_train, numeric_data_test = train_test_split(numeric_data, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2ae80-d076-489e-af05-251d81820da3",
   "metadata": {
    "id": "automatic-candy"
   },
   "source": [
    "##### Normalizing the data\n",
    "\n",
    "Before training our linear regression model, we have to normalize the data. We do this by subtracting each column from its minimum value and then dividing it by the difference between maximum and minimum. We use the `MinMaxScaler` of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea634322-702e-42d7-aaa3-722b144ac6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "#Fit the scaler\n",
    "scaler.fit(numeric_data_train)\n",
    "#Transform the train and the test set\n",
    "numeric_data_train.loc[:,:] = scaler.transform(numeric_data_train)\n",
    "numeric_data_test.loc[:,:] = scaler.transform(numeric_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce94995-d1aa-4efc-9bc9-727c32d23291",
   "metadata": {},
   "source": [
    "Finally, we split the column we want to predict (\"SalePrice\") to our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d211f0-ee9d-4ded-bfb0-c870e91ff0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and output\n",
    "numeric_x_columns = list(numeric_data_train.columns)\n",
    "numeric_x_columns.remove(\"SalePrice\")\n",
    "X_train_df = numeric_data_train[numeric_x_columns]\n",
    "y_train_df = pd.DataFrame(numeric_data_train[\"SalePrice\"])\n",
    "X_test_df = numeric_data_test[numeric_x_columns]\n",
    "y_test_df = pd.DataFrame(numeric_data_test[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91b5e4-c62a-48eb-9ded-7b3799c69f85",
   "metadata": {},
   "source": [
    "Ok, all set, we can start building our Neural Net!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69796e1-d7d7-4cf4-9950-395082f2084d",
   "metadata": {
    "id": "sensitive-notion"
   },
   "source": [
    "#### Building a Linear Regression model with PyTorch\n",
    "\n",
    "We use the `PyTorch` library ([Documentation](https://pytorch.org/), imported at the beginning of this notebook with the following lines of codes:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "```\n",
    "\n",
    "`torch.nn` contains the building blocks to build Neural Nets, e.g., the layers ([Documentation](https://pytorch.org/docs/stable/nn.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b7976-ea1e-4a95-9e53-c29d816e615d",
   "metadata": {},
   "source": [
    "##### Create tensors\n",
    "\n",
    "The first step is to convert the data into torch tensors. A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type. It's very similar to arrays in `NumPy`.\n",
    "\n",
    "We rely on `torch.tensor()` for the conversion ([Documentation](https://pytorch.org/docs/stable/tensors.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad283baf-f652-4d6f-a6fa-441c53731ea2",
   "metadata": {
    "id": "ranging-hudson"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train_df.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train_df.values, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test_df.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test_df.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abafe50-c9cf-4275-8d08-3aca51122e0f",
   "metadata": {
    "id": "prescribed-reason"
   },
   "outputs": [],
   "source": [
    "print(X_train.size(), y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae66a72-8269-4917-9ac4-0c42f8dcd249",
   "metadata": {
    "id": "baking-jamaica"
   },
   "source": [
    "##### Define and train a model with PyTorch\n",
    "\n",
    "A model is defined as a `class` in PyTorch. Classes are a means of bundling data and functionality together, allowing to create a new type of Python object. You can read the Python documentation on [Classes](https://docs.python.org/3/tutorial/classes.html) to learn more about them.\n",
    "\n",
    "When you create your Neural Net, you should define:\n",
    "- a `__init__` function in which you define the layers of your network. \n",
    "- a `forward` function (method) that defines the forward pass on the network.\n",
    "\n",
    "For the beginning, let's start with a single layer network:\n",
    "- The layer `nn.Linear()` performs a linear transformation ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)). The input and output are the number of neurons\n",
    "- `nn.ReLU()` applies the Rectified Linear Unit function: $ReLU(x)=\\max(0,x)$ ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e00f35-13ce-4e0b-946c-a2f0ad780e79",
   "metadata": {
    "id": "written-motel"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer\n",
    "        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer\n",
    "        self.activation = nn.ReLU()               # Activation function for hidden layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU\n",
    "        y_pred = self.linear2(y_pred)               # Output layer: linear transformation\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb4e35-61d3-49fe-aa50-a0c9e7590be9",
   "metadata": {},
   "source": [
    "`D_in` is the input dimension, i.e., the number of features. Similarly, `D_out` is the output dimension, i.e., 1 (we only predict the \"SalePrice\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3176e-1cab-4b11-88c6-f3547752aca3",
   "metadata": {
    "id": "sustainable-blade"
   },
   "outputs": [],
   "source": [
    "D_in, D_out = X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be40cbf-4811-4338-afab-2de1798c6257",
   "metadata": {},
   "source": [
    "Ok, let's define our first model. It is an instance of our newly-created class \"Net\". We are going to use 500 neurons for the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c315b3f-951b-4b8b-8e5b-12594dcc265f",
   "metadata": {
    "id": "subjective-camcorder"
   },
   "outputs": [],
   "source": [
    "# Model with 500 neurons\n",
    "model1 = Net(D_in, 500, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c9515-d808-4838-92d1-a56deb34ff08",
   "metadata": {
    "id": "sticky-flash"
   },
   "source": [
    "The next steps is to define the **loss criterion** and the **optimizer** for the network. That is, we have to define the loss function we want to optimize during training and also the optimization method. We use:\n",
    "- `MSELoss()` as loss criterion, i.e., the mean square error ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html))\n",
    "- `SGD()`as optimizer, i.e., stochastic gradient descent ([Documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efef490-e70e-445e-be89-3f206912d03e",
   "metadata": {
    "id": "permanent-fancy"
   },
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4268da0-fa53-43e2-8ee8-e8f3bb952d22",
   "metadata": {
    "id": "individual-gentleman"
   },
   "source": [
    "Wonderful, we are ready to do the training! We can simply by looping over the number of iterations. The training has 3 main steps:\n",
    "- A forward pass to compute the prediction for the current data point (batch).\n",
    "- Computing the loss for the current prediction with the previously defined criterion.\n",
    "- A backward pass to compute the gradient of the loss with respect to the weight of the network (`backward()`)\n",
    "- Finally, updating the weights of the network (`optimizer.step()`).\n",
    "\n",
    "Note that in each backward pass PyTorch saves the gradient for all of the parameters. Therefore it is important to replace the old gradient values with zero in the beginning of each iteration (`optimizer.zero_grad()`), otherwise the gradients will be accumulated during the iterations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec6c24-99e9-4a0f-bebd-25ea390edd0b",
   "metadata": {
    "id": "earlier-maximum"
   },
   "outputs": [],
   "source": [
    "losses1 = []\n",
    "losses1_test = []\n",
    "\n",
    "for t in range(500):                # 500 iterations\n",
    "    \n",
    "    # Forward pass: compute prediction on training set\n",
    "    y_pred = model1(X_train)  \n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    # Compute gradient\n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute loss on test set\n",
    "    losses1_test.append(criterion(model1(X_test), y_test).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d02e07-bd34-45ac-867d-8982849127a0",
   "metadata": {},
   "source": [
    "Let's visualize the evolution of the MSE on the training set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f20a67-f7f3-457b-be47-fa0884f43921",
   "metadata": {
    "id": "KG4HrZFfhXZs"
   },
   "outputs": [],
   "source": [
    "# Plot training and test loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(losses1, label=\"Training loss\")\n",
    "plt.plot(losses1_test, label=\"Test loss\")\n",
    "plt.title('Evolution of training and test loss - 500 neurons')\n",
    "plt.ylim(top=70, bottom=0.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d9df0-6281-4f7e-bc24-de4043050d9f",
   "metadata": {
    "id": "coordinated-mexican"
   },
   "source": [
    "Now let's try a new model with more neurons in the hidden layer. We use 1000 neurons, and follow the same steps as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01fdf3-d136-4186-9031-5afa987dcff6",
   "metadata": {
    "id": "cathedral-exercise"
   },
   "outputs": [],
   "source": [
    "model2 = Net(D_in, 1000, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff1abe-a077-4aae-ac9b-3f691384d92d",
   "metadata": {
    "id": "exposed-league"
   },
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66ea8a-0024-4a15-a610-b2a48bffb53f",
   "metadata": {
    "id": "operational-paper"
   },
   "outputs": [],
   "source": [
    "losses2 = []\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model2(X_train)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses2.append(loss.item())\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a867ebe-fc63-48e5-bf2c-4c0f04c4f507",
   "metadata": {},
   "source": [
    "Let's visualize the evolution of the training loss for the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c835d-cadf-4149-83f0-36356f48a67a",
   "metadata": {
    "id": "extra-consortium"
   },
   "outputs": [],
   "source": [
    "# Plot training and test loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(losses1, label=\"Model 1: 500 neurons\")\n",
    "plt.plot(losses2, label=\"Model 2: 1000 neurons\")\n",
    "plt.title('Evolution of training loss')\n",
    "plt.ylim(top=70, bottom=0.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da286f3-4298-4bac-901a-a553314f7201",
   "metadata": {
    "id": "banned-honolulu"
   },
   "source": [
    "Let's compare the MSE loss on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7035489-609d-423f-a36c-0158923fb064",
   "metadata": {
    "id": "parallel-following"
   },
   "outputs": [],
   "source": [
    "# prediction for model 1\n",
    "model1_pred = model1(X_test)\n",
    "print(\"MSE loss for model 1: \", criterion(model1_pred, y_test))\n",
    "# prediction for model 2\n",
    "model2_pred = model2(X_test)\n",
    "print(\"MSE loss for model 2: \", criterion(model2_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd45a42-8bdc-45e6-be68-7e18b7253a63",
   "metadata": {},
   "source": [
    "What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8367747-4cbe-45e1-8003-b4ba277f2741",
   "metadata": {},
   "source": [
    "#### Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b341a-0247-44e5-bb79-fcc15a351b0d",
   "metadata": {},
   "source": [
    "- Let's get back to model1. This time try to train it with a new optimizer. Try the Adam optimizer (which has shown to be faster than SGD for non-convex functions) and compare the training loss curve with SGD by plotting the training loss for the model trained with SGD and Adam optimizer.\n",
    "\n",
    "Note: Use `torch.optim.Adam(model1.parameters(), lr=...)` ([Documentation](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d084aa-b853-4ea9-8680-98b31cba5e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f169d7-4221-4e1d-b62a-dae28963c139",
   "metadata": {},
   "source": [
    "- This time we want to build a new model with a new architecture. Specifically, we want to train a network with 3 hidden layers on the data. You can use the following code to build the architecture. Use the values 500, 1000, 200 for H1, H2, and H3 respectively. Train this new network on the same training data and compare it with the model1 we built above.\n",
    "\n",
    "```\n",
    "class Net_new(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super(Net_new, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))\n",
    "        y_pred = self.activation(self.linear2(y_pred))\n",
    "        y_pred = self.activation(self.linear3(y_pred))\n",
    "        y_pred = self.linear4(y_pred)\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1303a-7a43-4bcc-a930-b91d16acf1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d59c54-95d3-4781-ac65-68ab73e626ac",
   "metadata": {},
   "source": [
    "## Implement a pre-built Neural Net\n",
    "\n",
    "One humongous strength of the Data Science community and of Python is the open-source values shared by its members. Hence, there exist many pre-built models for various tasks that are available for you to reuse! Using pre-trained models can reduce your computer costs, carbon footprint, and save you the time and resources required to train a model from scratch! \n",
    "\n",
    "[Hugging Face](https://huggingface.co/) is one of those AI community. It contains almost 200'000 [models](https://huggingface.co/models) and more than 30'000 [datasets](https://huggingface.co/datasets)!\n",
    "\n",
    "In a few lines of codes, you can load and use pre-built models using:\n",
    "- `transformers`, a library with state-of-the-art Machine Learning for PyTorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. Check the [Documentation](https://huggingface.co/docs/transformers/index)!\n",
    "- the [Hosted Inference API](https://huggingface.co/docs/api-inference/index), an API to test and evaluate, for free, over 80'000 publicly accessible machine learning models, via simple HTTP requests\n",
    "\n",
    "Let's try it! We will load a model using `transformers`. From this library, we import `pipeline()` (at the beginning of this notebook):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline \n",
    "```\n",
    "\n",
    "We will load the model \"[bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)\", i.e., the [BART (large-sized model)](https://huggingface.co/facebook/bart-large). BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n",
    "\n",
    "The model is pre-trained on English language, and more precisely on the Multi-Genre Natural Language Inference ([MultiNLI](https://huggingface.co/datasets/multi_nli)) dataset. This corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. \n",
    "\n",
    "Ok, let's go! The model is loaded with the \"zero-shot-classification\" pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743338c5-f1ed-451f-8adc-1430de70c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd8a1a-f751-4a7b-b3db-c5e2d37b00d7",
   "metadata": {},
   "source": [
    "[Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification) is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.\n",
    "\n",
    "In other words, we can provide to our classifier a sentence to classify and a list of labels, and the model with predict the class the sentence belongs to. For instance, let's say our classes are some SDGs, e.g., SDG 1 \"No Poverty\", SDG 5 \"Gender Equality\", and SDG 13 \"Climate Action\", and we want to classify the sentence: \"In Switzerland, the greenhouse gases emissions per capita are about 14 tonnes of CO2\". It's very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c70eb0-9311-49a7-942a-7ea0e938ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"In Switzerland, the greenhouse gases emissions per capita are about 14 tonnes of CO2.\"\n",
    "candidate_labels = ['No Poverty', 'Gender Equality', 'Climate Action']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac095342-f38e-4ea3-b451-ca9609cfebc6",
   "metadata": {},
   "source": [
    "The model correctly predicted the sentence as \"Climate Action\".\n",
    "\n",
    "To further improve the performance of the model for your specific task, you can **fine-tune a pre-trained model** with your own dataset in PyTorch or in TensorFlow with Keras. Here is a [tutorial](https://huggingface.co/docs/transformers/training). You can also check the post [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/sentiment-analysis-python) by Federico Pascual for a specific guide on sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4991d-f7b5-4e15-a7af-e700cbab93cf",
   "metadata": {},
   "source": [
    "Finally, as mentioned above, instead of using the `transformers` library, you can also rely on the Hosted Inference API, by making a request (you need to sign up to get an API token).\n",
    "\n",
    "You can use the following lines of codes:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"In Switzerland, the greenhouse gases emissions per capita are about 14 tonnes of CO2.\",\n",
    "    \"parameters\": {\"candidate_labels\": ['No Poverty', 'Gender Equality', 'Climate Action']},\n",
    "})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
