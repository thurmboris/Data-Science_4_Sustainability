{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "# Import for text analytics\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "\n",
    "<img src='https://images.unsplash.com/photo-1605429201125-37e867327609?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1176&q=80' width=\"450\">\n",
    "\n",
    "Credit: [Piotr Łaskawski](https://unsplash.com/@tot87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onO46LysT2dh"
   },
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on text analytics. [Text Analytics](https://en.wikipedia.org/wiki/Text_mining) (or text mining) is \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" ([Marti Hearst](https://people.ischool.berkeley.edu/~hearst/text-mining.html)). Written resources may include websites, books, emails, reviews, and articles.\n",
    "\n",
    "There are many applications of text analytics, for example:\n",
    "- Search for relevant websites or articles using a search engine;\n",
    "- Sentiment Analysis (e.g., classify tweets or film reviews as positive, neutral or negative);\n",
    "- Summarize, anonymize, or translate documents;\n",
    "- Chatbots (e.g., ChatGPT, Siri, Alexa);\n",
    "- etc.\n",
    "\n",
    "In this notebook, we will see how to prepare and represent texts and explore various text-analytics techniques, before doing an application on text similarity:\n",
    "- [Text Preparation](#Text-Preparation)\n",
    "    - [Tokenization](#Tokenization)\n",
    "    - [Remove Stopwords](#Remove-Stopwords)\n",
    "    - [Lemmatization](#Lemmatization)\n",
    "    - [Your turn!](#Your-turn-Preparation)\n",
    "- [Text structure](#Text-structure)\n",
    "    - [Dependency Parsing](#Dependency-Parsing)\n",
    "    - [Entity Detection](#Entity-Detection)\n",
    "    - [Your turn!](#Your-turn-structure)\n",
    "- [Text Representation](#Text-Representation)\n",
    "    - [Bag of Words (BOW)](#Bag-of-Words-(BOW))\n",
    "    - [TF-IDF Representation](#TF-IDF-Representation)\n",
    "    - [Your turn!](#Your-turn-Representation)\n",
    "- [Application: Text Similarity](#Application:-Text-Similarity)\n",
    "    - [Converting text into vectors](#Converting-text-into-vectors)\n",
    "    - [Cosine similarity](#Cosine-similarity)\n",
    "    - [Your turn!](#Your-turn-Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXlVy05IUC-D"
   },
   "source": [
    "## Text Preparation\n",
    "\n",
    "In this section, we explain how to prepare a text for analysis. This includes tokenizing the text, removing stopwords, etc. \n",
    "\n",
    "We will use the [spaCy](https://spacy.io/) library, an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently.\n",
    "\n",
    "You can directly [install the library](https://spacy.io/usage) in your Anaconda environment, or, if you opened this notebook in Colab, with the following line of code: \n",
    "```python\n",
    "!pip install -U spacy\n",
    "```\n",
    "\n",
    "We also install the English-language model: in you Anaconda environment install \"spacy-model-en_core_web_sm\"; in Colab, run the following line of code: \n",
    "```python\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Note: If you obtain the error `Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\"`, try the following: \n",
    "```python\n",
    "!pip --trusted-host github.com --trusted-host objects.githubusercontent.com install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is installed, and imported (at the beginning of this notebook), we can load our language dictionary, namely the English language model, using `spacy.load('en_core_web_sm')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUibE-SyTp8d"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "**Tokenization** is the process of breaking a text into pieces called tokens. A **token** simply refers to an individual part of a sentence having some semantic value. In other words, tokens are the elementary building blocks (words, numbers, characters) in a document. \n",
    "\n",
    "SpaCy's tokenizer takes input in form of unicode text and outputs a sequence of token objects. In addition, SpaCy automatically breaks your document into tokens when a document is created using the language model.\n",
    "\n",
    "There are a couple of different ways we can approach this. The first is called **word tokenization**, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require inputs in the form of individual words rather than longer strings of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at a simple example. Imagine we have the following text, and we would like to tokenize it:\n",
    "\n",
    "> When learning data science, you shouldn't get discouraged!\n",
    "\n",
    "> Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
    "\n",
    "We create a spaCy object, which contains linguistic annotations and various language properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dj9OfB1BTp8e",
    "outputId": "f2c13015-b33d-416f-bcf9-0299e572c720"
   },
   "outputs": [],
   "source": [
    "# Declare text\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "# spaCy object is used to create a document\n",
    "my_doc = sp(text)\n",
    "\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hc7fwTbPZdXJ",
    "outputId": "7ef4ce1e-7219-47f4-8f27-9b03fffaf77b"
   },
   "outputs": [],
   "source": [
    "# This is a spaCy document\n",
    "type(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ex7-T-BkYXZM",
    "outputId": "300dae65-c000-4b90-fdb2-b21dd474742a"
   },
   "outputs": [],
   "source": [
    "# Create list of tokens\n",
    "token_list = [token.text for token in my_doc]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi29AcSsTp8j"
   },
   "source": [
    "As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as _shouldn’t_ actually represent two distinct words, and has thus broken them down into two distinct tokens.\n",
    "\n",
    "We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvkGwDJoWoIs",
    "outputId": "9459553d-e026-43b8-ba5b-3fe104a62687"
   },
   "outputs": [],
   "source": [
    "# POS\n",
    "for word in my_doc:\n",
    "    print(word.text, '->', word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUsKHgp7Y3yM",
    "outputId": "ef4d07ce-c42c-4094-934a-66513f630bb7"
   },
   "outputs": [],
   "source": [
    "# Another example\n",
    "doc1 = sp(\"I like to fish\") # verb\n",
    "doc2 = sp(\"I eat a fish\") # noun\n",
    "\n",
    "for word in doc1:\n",
    "    print(word.text, '->', word.pos_)\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "for word in doc2:\n",
    "    print(word.text, '->', word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCiU1-PDXKpp"
   },
   "source": [
    "If we want, we can also break the text into sentences rather than words. This is called **sentence tokenization**. When performing sentence tokenization, the tokenizer looks for specific characters that normally fall between sentences, like periods, exclamation points, and newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odi_WkNZTp8k",
    "outputId": "740de7d3-9f7b-47c1-da0f-94e15648ef07"
   },
   "outputs": [],
   "source": [
    "# create list of sentence tokens\n",
    "sents_list = [sent.text for sent in my_doc.sents]\n",
    "\n",
    "sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_Z6MiCtTp8n"
   },
   "source": [
    "### Remove Stopwords\n",
    "\n",
    "Most text data that we work with is going to contain a lot of words that are not actually useful to our analysis (e.g., \"is\", \"and\", \"you\", etc.). These words, called **stopwords**, are useful in human speech, but they do not have much to contribute to the meaning of a sentence. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time of the analysis (since there are fewer words to process). This makes text analysis more efficient.\n",
    "\n",
    "\n",
    "Let’s take a look at the stopwords spaCy includes by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kB8kI0eTp8o",
    "outputId": "0d980a3e-3b63-471b-f761-3ef205f7d2f6"
   },
   "outputs": [],
   "source": [
    "# Import stopwords from English language\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Print total number of stopwords\n",
    "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
    "\n",
    "# Print 20 stopwords\n",
    "print('20 stopwords: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw0Wfvu4Tp8q"
   },
   "source": [
    "Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eq5yvoZtlhsd",
    "outputId": "8566b4f1-3c53-4650-a193-3c9a89ffe276"
   },
   "outputs": [],
   "source": [
    "# Which words will be removed?\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y54Kiz9zTp8r",
    "outputId": "45d56d1f-76e5-4270-9757-f142ce8b5656"
   },
   "outputs": [],
   "source": [
    "# Filter stopwords\n",
    "filtered_sent = [word.text for word in my_doc if word.is_stop == False]\n",
    "\n",
    "print('The filtered sentence contains the words:', filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove the punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOvM2NFdl29W",
    "outputId": "9f7b84e2-4cbe-4a08-87f6-f764119ae81e"
   },
   "outputs": [],
   "source": [
    "# Filter stopwords, punctuation and spaces\n",
    "filtered_sent2 = []\n",
    "removed_tokens = []\n",
    "\n",
    "for word in my_doc:\n",
    "    if (word.is_stop == True) or (word.is_punct == True) or (word.is_space == True):\n",
    "        removed_tokens.append(word.text)\n",
    "    else:\n",
    "        filtered_sent2.append(word.text)\n",
    "\n",
    "print('We remove the following tokens:', removed_tokens)\n",
    "print('The filtered sentence contains the words:', filtered_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imbw_TUkTp8v"
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "**Lemmatization** is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. are not exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\n",
    "\n",
    "One method for doing this is called **stemming**. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what is often the simplest version of a word, the root. Connection, for example, would have the -ion suffix removed and be reduced to connect. This kind of simple stemming is often all that is needed, but lemmatization — which actually looks at words and their roots (called lemma) as described in the dictionary — is more precise (e.g feet -> foot).\n",
    "\n",
    "Let's look at this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTqSImYfTp8v",
    "outputId": "3f138cd1-f8e7-44eb-f1cf-509998d3efb6"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lem = sp(\"run runs ran running runner runners\")\n",
    "\n",
    "# Find lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text, '->', word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn! <a id = \"Your-turn-Preparation\"></a>\n",
    "\n",
    "The text below is taken from the [E4S report](https://e4s.center/resources/reports/true-cost-of-food-as-a-lever-to-transform-the-swiss-food-system/) on the *True cost of food as a lever to transform the Swiss food system*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"It is estimated that CHF 37.4 billion is spent every year on food in Switzerland. \n",
    "The estimated external costs are at least twice this amount, with health externalities weighing the most. \n",
    "Even if these costs are hidden from the market, they are incurred by our planet and societies, through e.g. public health costs and natural resources depletion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create two lists:\n",
    "    - the first one containing the punctuation and the stopwords,\n",
    "    - the second one containing the words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each token, print its lemma\n",
    "\n",
    "*Note:* You can convert a list of strings into a string using for instance the `join()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te403rkXbGKf"
   },
   "source": [
    "### Dependency Parsing\n",
    "\n",
    "**Dependency parsing** is a language processing technique that allows to better determine the meaning of a sentence by analyzing how it is constructed to determine how the individual words relate to each other.\n",
    "\n",
    "Consider, for example, the sentence “Florence throws the ball.” We have two nouns (Florence and ball) and one verb (throws). But we cannot just look at these words individually, or we may end up thinking that the ball throws Florence! To understand the sentence correctly, we need to look at the word order and sentence structure, not just the words.\n",
    "\n",
    "Below, we have a short sentence. We’ll use a spaCy method called `noun_chunks` ([Documentation](https://spacy.io/usage/linguistic-features#dependency-parse)), which breaks the input down into nouns and the words describing them, and iterate through each chunk in our source text, identifying the word, its root, its dependency identification, and which chunk it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFLButK_bOor",
    "outputId": "b49f7fdc-8790-48fa-ed07-ff780dc20992"
   },
   "outputs": [],
   "source": [
    "doc = sp(\"Florence threw a ball, and her friend Edoardo, in pursuit of the ball, hit a wall.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, '-' , chunk.root.text, '-', chunk.root.dep_, '-', chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this to better understand how our sentence is structured. We use a spaCy visualizer ([Documentation](https://spacy.io/usage/visualizers)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7VmqTQQ9bYTl",
    "outputId": "0aa614eb-1543-49ec-dbb1-5761a7455ec1"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "displacy.render(doc, style=\"dep\", jupyter= True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the tags are pretty abstract (and they vary between languages), you can use `spacy.explain` to get a short description of that the tag stands for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['nsubj', 'dobj', 'det', 'cc', 'conj', 'poss', 'appos', 'prep', 'pobj',\n",
    "       'PROPN', 'VERB', 'DET', 'NOUN', 'CCONJ', 'ADP']\n",
    "\n",
    "@interact\n",
    "def spacy_explain(dep = tags):\n",
    "    return spacy.explain(dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7J5Na4DTp84"
   },
   "source": [
    "### Entity Detection\n",
    "\n",
    "Entity detection, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within a text. This is really helpful for quickly extracting information from the text, since you can quickly pick out important topics or identify key sections of it.\n",
    "\n",
    "SpaCy, comes with a pre-trained classifier that detects important entities: location, time, people, money etc. To get the named entities from a document, you have to use the `ents` attribute. \n",
    "\n",
    "Let’s try out some entity detection using sentences about the life of [Marie Curie](https://en.wikipedia.org/wiki/Marie_Curie):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvlb9S71Tp84",
    "outputId": "713e6602-f65d-425e-801c-ee50637847fd"
   },
   "outputs": [],
   "source": [
    "article = sp(\"\"\"Marie Curie was a Polish and naturalized-French physicist and chemist. She was born in Warsaw in 1867 and moved to France in 1891.\n",
    "She was the first person to be awarded two Nobel Prizes by the Royal Swedish Academy of Sciences: in physics in 1903 for her pioneering research on radioactivity and in chemistry in 1911 for the discovery of the elements polonium and radium.\n",
    "During World War I she developed mobile radiography units to provide X-ray services to field hospitals.\"\"\")\n",
    "\n",
    "entities = [(i, i.label_, i.label) for i in article.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1ScYkq7Tp87"
   },
   "source": [
    "The above example how spaCy is able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc.\n",
    "\n",
    "Using `displaCy` we can also visualize the text, with each identified entity highlighted by a color and labeled. We’ll use `style=\"ent\"` to tell displaCy that we want to visualize entities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "hGDOBT6zTp88",
    "outputId": "f3056b84-afc8-4c1a-d198-8d6ad75fa4d7"
   },
   "outputs": [],
   "source": [
    "displacy.render(article, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erd3X6Q-n1_Y"
   },
   "source": [
    "### Your turn! <a id = \"Your-turn-structure\"></a>\n",
    "\n",
    "- Display the entities in the sentences below, about the life of [Elinor Ostrom](https://en.wikipedia.org/wiki/Elinor_Ostrom):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyJOSspCn7W6"
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"Elinor Olstrom was an American political scientist and political economist born in Los Angeles. In 2009, she became the first woman to receive the Nobel Memorial Prize in Economic Sciences for her analysis of economic governance, especially the commons. \n",
    "By conducting field studies, she showed that, contrary to popular beliefs on the tragedy of the commons, natural resources that are collectively used by their users are not over-exploited and destroyed in the long-term. Instead, local communities manage shared natural resources (e.g., pastures, fishing waters, and forests) by establishing rules so that resources are used in a way that is both economically and ecologically sustainable.\n",
    "Her research is described in her book \"Governing the Commons: The Evolution of Institutions for Collective Action\".\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZOutDwsYij"
   },
   "source": [
    "## Text Representation\n",
    "\n",
    "The goal is to transform text into numerical features such that it can be used by ML algorithms. There are different techniques:\n",
    "- **Bag of Words (BOW)** simply treat every document as an unordered set of words. It works in many case but order is not preserved. As a solution, we can use **n-grams**, i.e., we count token pairs, triplets, etc.\n",
    "- **TF-IDF**: emphasizes important words, i.e., words that appear frequently in a document, (informing about the topic of the document), and words that are rare in a corpus of documents (setting one document apart from other similar ones).\n",
    "\n",
    "To transform our text, we are going to use our old friend, the scikit learn library. As input it will require strings (and not a spaCy object). Here, our corpus of documents will consist of four sentences on [symbiosis](https://en.wikipedia.org/wiki/Symbiosis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2MjeqoQxdOx"
   },
   "outputs": [],
   "source": [
    "# Sentences (as strings, not spaCy objects)\n",
    "s1 = \"Symbiosis is any type of a close and long-term biological interaction between two biological organisms of different species.\"\n",
    "s2 = \"Mutualism describes the ecological interaction between two or more species where each species has a net benefit.\"\n",
    "s3 = \"Commensalism is a long-term biological interaction (symbiosis) in which members of one species gain benefits while those of the other species neither benefit nor are harmed.\"\n",
    "s4 = \"Parasitism is a close relationship between species, where one organism, the parasite, lives on or inside another organism, the host, causing it some harm, and is adapted structurally to this way of life.\"\n",
    "\n",
    "# List of sentences\n",
    "texts = [s1, s2, s3, s4]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCFQrtHLtMTt"
   },
   "source": [
    "### Bag of Words (BOW)\n",
    "\n",
    "We use the `CountVectorizer` class of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)), using as parameters:\n",
    "- `ngram_range=(1,2)`, i.e., we consider tokens (1-grams) and pair of tokens (2-grams);\n",
    "- `stop_words=\"english\"`, a built-in stop word list for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyzBm0BUtLc0",
    "outputId": "231db08a-925c-440c-a642-90d03ad5921b"
   },
   "outputs": [],
   "source": [
    "# Using default tokenizer \n",
    "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "# Learn the vocabulary dictionary and return document-term matrix\n",
    "bow = count.fit_transform(texts)\n",
    "\n",
    "# Show feature matrix\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the n-grams (tokens and pair of tokens) created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRmUUc0kzi3_",
    "outputId": "a3648c86-8ed3-48db-bdc7-61cac70236ea"
   },
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "# View feature names\n",
    "print('Our n-grams are:', ', '.join(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better visualize the result in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "sxCSvGUTz2tl",
    "outputId": "2b1273d6-433d-4f79-8d29-d07fc8e2d7a7"
   },
   "outputs": [],
   "source": [
    "# Show as a dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.DataFrame(\n",
    "    bow.todense(),              # Feature matrix\n",
    "    columns=feature_names,      # n-grams\n",
    "    index= ['s1', 's2', 's3', 's4']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7tNGrTutQ2r"
   },
   "source": [
    "### TF-IDF Representation\n",
    "\n",
    "**TF-IDF** emphasizes important words. It is the product of term frequency (TF) and inverse document frequency (IDF):\n",
    "- **Term Frequency** identifies tokens that appear frequently in a document: TF(token, document) = number of times token appears in document / total number of tokens in document\n",
    "- **Inverse Document Frequency** identifies words that appear rarely in the corpus: IDF(token, corpus) = log( total number of documents in corpus / number of documents containing token )\n",
    "\n",
    "Note that the IDF value for a token remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a token differ from document to document.\n",
    "\n",
    "The goal of using TF-IDF instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus since those are less informative than tokens that occur in a small fraction of the corpus. \n",
    "\n",
    "Ok, let's implement TD-IDF. We are using the `TfidfVectorizer` class of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "djkUI6hXtWlr",
    "outputId": "26cb28a5-eb81-40ff-ebbc-48d5a6d4a621"
   },
   "outputs": [],
   "source": [
    "# Using default tokenizer in TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
    "\n",
    "# Learn the vocabulary dictionary and return document-term matrix\n",
    "features = tfidf.fit_transform(texts)\n",
    "\n",
    "# Visualize result in dataframe\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index = ['s1', 's2', 's3', 's4']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcrRL3rRqgBy"
   },
   "source": [
    "### Your turn! <a id = \"Your-turn-Representation\"></a>\n",
    "\n",
    "- Create a TF-IDF Representation of the three sentences below using bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences\n",
    "s1 = \"The linear economy, sometimes referred to as the take-make-waste economy, is a system where resources are extracted to make products that eventually end up as waste and are thrown away.\"\n",
    "s2 = \"A circular economy is a regenerative economic system that uses renewable energy and resources, reuses materials and products as long as possible, and recycles resources rather than disposing them as waste.\"\n",
    "s3 = \"A green economy is an economy that aims at reducing environmental risks and ecological scarcities, and that aims for sustainable development without degrading the environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzo8uviDrTLm"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Text Similarity <a id = \"Your-turn-Similarity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this application, we will compute the similarity between various texts. We'll use Wikipedia articles, relying on the `wikipedia` library ([Documentation](https://pypi.org/project/wikipedia/)) to avoid doing the preprocessing, e.g., web scrapping to extract the articles. You can install the library using `pip install wikipedia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"Albert Einstein\",\n",
    "          \"Adam Smith\",\n",
    "          \"Alexander von Humboldt\",\n",
    "          \"Amartya Sen\",\n",
    "          \"Daniel Kahneman\",\n",
    "          \"John von Neumann\",\n",
    "          \"Joseph Stiglitz\",\n",
    "          \"Stephen Hawking\"\n",
    "]\n",
    "\n",
    "# Extract summary of articles\n",
    "data_wiki = [wikipedia.summary(topics[p], sentences=20) for p in range(len(topics))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn a bit more about [Alexander von Humboldt](https://en.wikipedia.org/wiki/Alexander_von_Humboldt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wiki[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"father of ecology\", not bad! Ok, let's start our similarity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text into vectors\n",
    "\n",
    "The first step is to convert our articles into numerical features. We use `CountVectorizer`, i.e., the Bag of Words technique, with tokens and pair of tokens (2-grams), and without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default tokenizer \n",
    "count_wiki = CountVectorizer(ngram_range=(1,2), stop_words = None)\n",
    "bow_wiki = count_wiki.fit_transform(data_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "We can now compute the **cosine similarity**. We use the `cosine_similarity` module of sklearn ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise similarities\n",
    "pairwise_similarities = cosine_similarity(bow_wiki, bow_wiki)\n",
    "\n",
    "# Visualize result in dataframe\n",
    "pairwise_df = pd.DataFrame(\n",
    "    pairwise_similarities,\n",
    "    columns = topics,\n",
    "    index = topics\n",
    ")\n",
    "pairwise_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a heatmap to obtain a more visual representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of pairwise similarities\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(\n",
    "    pairwise_df,\n",
    "    cmap='OrRd',\n",
    "    linewidth=1,\n",
    "    annot=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Let's continue the exploration of similarities between our articles.\n",
    "\n",
    "- Compute a heatmap of the pairwise similarities, varying the vectorization technique (`CountVectorizer`, `TfidfVectorizer`), the ngram range, and the stopwords (e.g., `None`, \"english\"). How do your model compares?\n",
    "\n",
    "*Note:* you could define a function and try interactive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Repeat the previous operations, but this time using the full text for each Wikipedia entry. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_wiki = [wikipedia.page(topics[p]).content for p in range(len(topics))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
