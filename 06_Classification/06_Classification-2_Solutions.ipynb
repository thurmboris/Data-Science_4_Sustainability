{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7329a5f4-685c-4885-bc20-b66b53cabad2",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/thurmboris/Data-Science_4_Sustainability/blob/main/06_Classification/06_Classification-2_Solutions.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Sklearn import\n",
    "from sklearn.model_selection import train_test_split # Splitting the data set\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler # Normalization and standard scaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Label and 1-hot encoding\n",
    "from sklearn.linear_model import LogisticRegression # Logistic regression model\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN Algorithm\n",
    "from sklearn.model_selection import GridSearchCV   # Grid search for cross validation\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree # Decision Trees\n",
    "from sklearn.metrics import accuracy_score  # Accuracy\n",
    "from sklearn.metrics import confusion_matrix # Confusion matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  # Precision, recall, and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Classification: K-Nearest Neighbours and Decision Tress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984e553-36af-42c8-9522-dd090686c913",
   "metadata": {},
   "source": [
    "<img src='https://i.postimg.cc/QxntZgdL/Supervised-Learning.jpg' width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on classification, focusing on two algorithms: k-Nearest Neighbors and Decision Trees. After presenting the main concepts, you will be introduced to the techniques to implement the algorithms in Python. Finally, it will be your turn to practice, using an application on forest fires. \n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [K-Nearest Neighbors](#K-Nearest-Neighbors)\n",
    "    - [Background](#KNN-Background)\n",
    "        - [Intuition](#KNN-Intuition)\n",
    "        - [Distance metric](#KNN-Distance-metric)\n",
    "        - [What is the right value of neighbors?](#KNN-neighbors)\n",
    "    - [Implementation](#KNN-Implementation)\n",
    "        - [Splitting the dataset](#KNN-Splitting)\n",
    "        - [Building, training and evaluating our classifier](#KNN-Building)\n",
    "        - [Tuning parameters with cross-validation](#KNN-Tuning)\n",
    "- [Decision Trees](#DT)\n",
    "    - [Background](#DT-Background)\n",
    "        - [Intuition](#DT-Intuition)\n",
    "        - [Decision criteria](#DT-Decision)\n",
    "    - [Implementation](#DT-Implementation)\n",
    "        - [Building, training and evaluating our classifier](#DT-Building)\n",
    "        - [Tuning parameters with cross-validation](#DT-Tuning)\n",
    "        - [Visualize Tree](#DT-Visualize)\n",
    "- [Your turn](#Your-turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd28f-f040-4b1b-9864-6510d4ea6dbc",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:786/format:webp/0*S4-FHBGgizjGFUjZ.jpg' width=\"300\">\n",
    "\n",
    "Source: Afroz Chakure, [K-Nearest Neighbors (KNN) Algorithm](https://medium.datadriveninvestor.com/k-nearest-neighbors-knn-algorithm-bd375d14eec7), Published in DataDrivenInvestor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5570d-cf11-4ef6-90cd-f5bdef3a8d23",
   "metadata": {},
   "source": [
    "### Background <a id = \"KNN-Background\"></a>\n",
    "\n",
    "#### Intuition <a id = \"KNN-Intuition\"></a>\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm assumes that similar things are near to each other: “Birds of a feather flock together”. Following this assumption, in order to classify a point, we measure the distance to the nearest k instances of the training set, and let them vote. K is typically chosen to be an odd number to have a tiebreaker.\n",
    "\n",
    "<img src='https://miro.medium.com/max/1300/0*Sk18h9op6uK9EpT8.' width=\"400\">\n",
    "\n",
    "The KNN algorithm is very useful when there are non-linear decision boundaries. For example, consider the image below, displaying whether there is vegetation depending on latitude and longitude. A logistic regression would split our plane into two and thus would not be able to correctly predict that vegetation data points are located in the top right and bottom left quadrants. However, KNN classifiers would perform much better since vegetation (and non-vegetation) data points are grouped in clusters.\n",
    "\n",
    "<img src='https://miro.medium.com/max/374/1*-W7HOfNfWk5BeXgF5jao6g.png' width=\"300\">\n",
    "\n",
    "Note that the algorithm can be used for both classification and regression. You can read [The Basics: KNN for classification and regression](https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955) for intuition on how KNN can be applied for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02efbaf-3650-4e0d-a53d-2443169a111d",
   "metadata": {},
   "source": [
    "#### Distance metric <a id = \"KNN-Distance-metric\"></a>\n",
    "\n",
    "As mentioned above, the KNN algorithm relies on the notion of distance between observations. Which distance? There are several possibilities, the most popular one being the Euclidean distance:\n",
    "\n",
    "- [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), also known as L2 norm. In a plane, it is the shortest distance - . Imagine we have $d$ (real-valued) features and we wish to calculate the distance between two observations $\\boldsymbol{x_{1*}}=(x_{11}, ..., x_{1d})$ and $\\boldsymbol{x_{2*}}=(x_{21}, ..., x_{2d})$, the Euclidean distance will be:\n",
    "$$d_2(\\boldsymbol{x_{1*}}, \\boldsymbol{x_{2*}})= \\sqrt{(x_{11}-x_{21})^2 + ... + (x_{1d}-x_{2d})^2 }= \\sqrt{\\sum_{j=1}^d (x_{1j}-x_{2j})^2}$$\n",
    "The Euclidean distance is useful in low dimension, it does not work well in high dimensions and for categorical variables. It also ignores the similarity between features since each feature is treated as totally different from all the other features.\n",
    "\n",
    "- [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry), also known as L1 norm or \"Taxicab\". The idea is to travel the space the same way taxis would navigate the streets in a city like the island of Manhattan, known for its [grid plan](https://en.wikipedia.org/wiki/Grid_plan):\n",
    "$$d_1(\\boldsymbol{x_{1*}}, \\boldsymbol{x_{2*}})= \\sum_{j=1}^d |x_{1j}-x_{2j}|$$\n",
    "Manhattan distance is favored over Euclidean distance when we have many features (see for instance, Aggarwal, Hinneburg, & Keim paper [On the Surprising Behavior of Distance Metrics in High Dimensional Space](https://link.springer.com/chapter/10.1007/3-540-44503-x_27)).\n",
    "\n",
    "- [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) generalizes the Euclidean and Manhattan distance:\n",
    "$$d_1(\\boldsymbol{x_{1*}}, \\boldsymbol{x_{2*}})= \\left(\\sum_{j=1}^d |x_{1j}-x_{2j}|^p \\right)^{1/p}$$\n",
    "For $p=1$, we get the Manhattan distance. For $p=2$, we get the Euclidean distance. For $p$ reaching $\\infty$, we have $d_\\infty= \\min_j |x_{1j}-x_{2j}| $ \n",
    "\n",
    "- [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) is used to compare distance between strings. It measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. More on that when we will study Text Analytics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5141f7d-a97d-4cd1-8bb3-175e41dfc247",
   "metadata": {},
   "source": [
    "#### What is the right value of k (neighbors)? <a id = \"KNN-neighbors\"></a>\n",
    "\n",
    "We pick the proper k using **cross validation** using ONLY the training data! The test data is used for the final evaluation of our model.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a19ea-246b-4af3-bd04-885218a6ccff",
   "metadata": {},
   "source": [
    "### Implementation <a id = \"KNN-Implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f09734-4975-40ee-a62d-00469c374bb6",
   "metadata": {},
   "source": [
    "In this walkthrough, we will try to classify fruits based on their characteristics such as mass, width, height, and color. The dataset is available in the /data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c08bb7-8678-4ac8-940b-de90c236fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "url_fruit = 'https://raw.githubusercontent.com/thurmboris/Data-Science_4_Sustainability/main/data/Fruits.csv'\n",
    "fruits = pd.read_csv(url_fruit,sep=\";\")\n",
    "\n",
    "# Display first 10 observations\n",
    "display(fruits.head(10))\n",
    "\n",
    "# Shape of dataset\n",
    "print('The dataset has {} observations.'.format(fruits.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3f906-2e2c-48b7-8161-fe8489077e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fruits.fruit_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc4c75-8cd7-4e14-aef8-294f6010b288",
   "metadata": {},
   "source": [
    "We want to predict the class (`fruit label`) using the features 'mass', 'width', 'height', 'color_score':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cf0c4-1a3e-4631-8cb9-5c90e4f06bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "features = ['mass', 'width', 'height', 'color_score']\n",
    "X = fruits[features]\n",
    "y = fruits['fruit_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ef430-181a-455c-8709-28335523f063",
   "metadata": {},
   "source": [
    "Let's check the different fruits in our dataset, the associated label and number of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14a176-8166-4896-9108-bbe6a497a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.groupby(['fruit_name','fruit_label']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19b025-da6d-4ab8-89d7-a5b1fb59236c",
   "metadata": {},
   "source": [
    "We have balanced class: each fruit has 19 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40951247-2059-4136-8bd8-a76b5ecf07be",
   "metadata": {},
   "source": [
    "Finally, let's check the summary statistics of our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892ebe7-2f50-4b3d-a6ac-2f2a77658ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c957de-64da-47be-bb93-314e34d83b5f",
   "metadata": {},
   "source": [
    "Notice that we have quite different scales, e.g., between \"mass\" and \"color_score\". We will thus need to rescale our features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d3598-bd1a-4784-9fa1-d3666543beb9",
   "metadata": {},
   "source": [
    "#### Splitting the dataset <a id = \"KNN-Splitting\"></a>\n",
    "\n",
    "As always, the first step is to split our data into random training and test subsets. Recall that the training set is used to learn the parameters of our model while the test set is used to evaluate our predictions.\n",
    "\n",
    "We use the `train_test_split` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) of sklearn, imported with the following line of code (already done at the beginning of the notebook):\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1df1d6-8dea-4381-9e40-def39736658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeaf125-3e51-447c-867b-022fd3eda22a",
   "metadata": {},
   "source": [
    "#### Rescaling   <a id = \"KNN-Rescaling\"></a>\n",
    "\n",
    "When we have a dataset with features that have very distinct ranges, we might get biased results. We want the features to be in the same or similar range, which also helps the interpretation of the model parameters (weights). \n",
    "\n",
    "We therefore **normalize** the data. It involves transforming all values for a specific attribute so that they fall within a small specified range. We can use `StandardScaler()`, ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) `MinMaxScaler()` ([Documentation](https://scikit-learn.org/0.15/modules/generated/sklearn.preprocessing.MinMaxScaler.html)) or others for normalization.\n",
    "\n",
    "In our example we will normalize both our **train AND test data** using `MinMaxScaler()`. Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n",
    "\n",
    "**IMPORTANT**: When you normalize the train data, you need to do the same modification (here normalization) to the test data. In other words, you train your scaler on your training set, and apply the same transformation to the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523f038-7694-429f-b767-285064b92329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training set \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae23f3c-2dcc-49f8-96d5-98199c23520d",
   "metadata": {},
   "source": [
    "We can check the results of the normalization process: all values should be between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba90285-aca2-4490-9f39-4c4b5362e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train, columns=['mass', 'width', 'height', 'color_score']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2e11d-a89d-432b-a3bf-ab193a55c994",
   "metadata": {},
   "source": [
    "#### Building, training and evaluating our classifier <a id = \"KNN-Building\"></a>\n",
    "\n",
    "We implement a KNN algorithm to predict the class of our target variable using the **sklearn** module `KNeighborsClassifier()` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "We can specify various parameters:\n",
    "- `n_neighbors`= number of neighboring observations to use\n",
    "- `p`= determines the distance/similarity metric (\"p\" refers to the Minkowski distance). \n",
    "    - When p = 1, the Manhattan distance (l1-norm) is used, \n",
    "    - When p = 2 (default value), the Euclidean_distance (l2-norm) is used. \n",
    "- `weights`= determines how to weigh the neighboring observations. \n",
    "  - When set to `uniform` (default value): uniform weights. All points in each neighborhood are weighted equally. \n",
    "  - When set to `distance` : weight points by the inverse of their distance. In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "Please refer to the documentation for the full list of parameters.\n",
    "\n",
    "We will start by arbitrarily selecting k=7 neighbors, and default values for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7b29f-c29b-4773-8755-5187be81589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our model\n",
    "model_kNN = KNeighborsClassifier(n_neighbors=7, p=2, weights='uniform')\n",
    "\n",
    "# Fit our model\n",
    "model_kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026ccaf-84e9-4412-bb48-79b1fe9bebfb",
   "metadata": {},
   "source": [
    "Let's check the accuracy of our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77f73c-241d-499c-8799-2b47335a00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of KNN classifier on test set: {:.2f}'.format(model_kNN.score(X_test, y_test)))\n",
    "\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'.format(model_kNN.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fb71d-cb91-4d45-8445-e7e568bc8873",
   "metadata": {},
   "source": [
    "Not bad! \n",
    "\n",
    "Let's loop over the model parameters (number of neighbors, distance metric, weights) to check how the accuracy is affected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176743cb-0546-4710-a41a-bb52205030b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [1,3,5,7]\n",
    "p_values = [1,2,3]\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "for w in weights:\n",
    "    for p in p_values:\n",
    "        for n in n_neighbors:\n",
    "            model = KNeighborsClassifier(n_neighbors=n, p=p, weights=w)\n",
    "            model.fit(X_train, y_train)\n",
    "            acc_model = model.score(X_test, y_test)\n",
    "            print(f\"Accuracy of KNN classifier for k = {n}, p = {p}, weight = {w}: {acc_model:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9323c5-7da2-4af5-aaa8-e80c21260e57",
   "metadata": {},
   "source": [
    "The accuracy is higher when we add weights based on the distance. The performance of the distance metrics depends on the number of neighbors selected. Interestingly, the best accuracy seems to be obtained using 3 or 5 neighbors, but then decrease, maybe due to the limited amount of observations in our dataset. In the following, we will use cross-validation to find the \"optimal\" parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89101bf5-96b9-4026-abd8-58a46c6ad86c",
   "metadata": {},
   "source": [
    "#### Tuning parameters with cross-validation <a id = \"KNN-Tuning\"></a>\n",
    "\n",
    "We now want to find which parameters of our KNN algorithm are the optimal ones. We use Grid Search Cross Validation, the idea being to explore a grid of potential parameters. \n",
    "\n",
    "The associated sklearn module is `GridSearchCV()` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)), imported via the following line code:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "```\n",
    "\n",
    "As parameters, we need to provide our `estimator`, i.e., our model, and a grid of parameters. We can also specify the number of folds `cv` used for cross-validation. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d406e0-4183-48c4-8839-56a697e8e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to test\n",
    "grid = {'n_neighbors':np.arange(1,10),     # array from 1 to 10 neighbors\n",
    "        'p':np.arange(1,3),                # array from 1 to 3, distance metrics\n",
    "        'weights':['uniform','distance']   # weights\n",
    "       }\n",
    "\n",
    "# Define and fit model\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, grid, cv=7)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Hyperparameters:\", knn_cv.best_params_)\n",
    "print(\"Train Score: {:0.2f}\".format(knn_cv.best_score_))\n",
    "print(\"Test Score: {:0.2f}\".format(knn_cv.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc667b29-9dbc-436e-a4a5-ef0c2c8fddf1",
   "metadata": {},
   "source": [
    "No need for a complex model, only one neighbor and we get a perfect accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee02d36-e94a-43b6-a3b6-c3d6eaac1741",
   "metadata": {},
   "source": [
    "## Decision Trees  <a id = \"DT\"></a>\n",
    "\n",
    "<img src='https://regenerativetoday.com/wp-content/uploads/2022/04/dt.png' width=\"400\">\n",
    "\n",
    "Source: [Simple Explanation on How Decision Tree Algorithm Makes Decisions](https://regenerativetoday.com/simple-explanation-on-how-decision-tree-algorithm-makes-decisions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14273048-92d9-48a1-a974-71edda505929",
   "metadata": {},
   "source": [
    "### Background  <a id = \"DT-Background\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713dde7-504d-45c1-803c-7e0e2501b76f",
   "metadata": {},
   "source": [
    "#### Intuition <a id = \"DT-Intuition\"></a>\n",
    "\n",
    "Decision trees, as the name goes, use a tree-like model of decisions. At each node, the algorithm chooses a splitting rule (based on a feature) that maximizes the accuracy of the model. More precisely, at every split the algorithm maximizes a certain criterion previously given (e.g., Gini index, information gain). \n",
    "\n",
    "The objective of the algorithm is to find the simplest possible decision tree (i.e., only a few nodes = a small depth) with the highest accuracy. \n",
    "\n",
    "Consider the example below, where the objective is to classify if a person is fit or not. If we would have chosen another criterion for the root node (e.g., \"Exercises in the morning\" intead of \"Age<30\"), we could have ended up with a lower accuracy and/or more splits (i.e., a more complex tree). The same logic applies at each decision node, until we reach the leafs, i.e., the final decision.\n",
    "\n",
    "<img src='https://cdn.educba.com/academy/wp-content/uploads/2019/05/is-a-person-fit.png' width=\"300\">\n",
    "\n",
    "Decision Trees are simple to understand, interpret, and visualize. They can handle both numerical and categorical data, they do not require feature scaling and can deal with outliers. The algorithm is also good at at handling non-linearly separable data. \n",
    "\n",
    "As a drawback, Decision Trees suffer for a risk of overfitting, especially with large dataset since the tree might become too complex. They can also be unstable because small variations in the data might result in a completely different tree being generated. Potential solutions to avoid overfitting and get better performance include:\n",
    "- Rely on cross-validation to find the proper depth.\n",
    "- Building a collection of trees, i.e., a Random Forest - you can for instance read [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) for more explanation on the topic.\n",
    "\n",
    "Finally, note that, as for KNN, Decision Trees can be used for both classification and regression. You can read [Machine Learning Basics: Decision Tree Regression](https://towardsdatascience.com/machine-learning-basics-decision-tree-regression-1d73ea003fda) for a walk through on how to apply Decision Tree for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396ea1e-5d83-4722-9e47-62e1aea1463d",
   "metadata": {},
   "source": [
    "#### Decision criteria  <a id = \"DT-Decision\"></a>\n",
    "\n",
    "Growing a tree involves deciding on which features to choose and what conditions to use for splitting, along with knowing when to stop. How to do so? We need decision criteria, which evaluate the cost of a split, or alternatively, the \"purity\" of the selection. Here are some measures:\n",
    "\n",
    "- [Information Gain](https://en.wikipedia.org/wiki/Information_gain_(decision_tree)) measures... the information gained thanks to the split, relying on the notion of [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)):\n",
    "$$\\text{Entropy}= - \\sum_{i=1}^c p_i \\log_2(p_i)$$\n",
    "where $c$ is the number of class and $p_i$ is the probability of randomly selecting an observation in class $i$. Let's consider two classes \"0\" and \"1\" for simplicity: $\\text{Entropy} = - p_0 \\log_2(p_0) - p_1 \\log_2(p_1)$:\n",
    "    - When our dataset (or node) has 50% of observations belonging to class \"0\" and 50% belonging to class \"1\", then $p_0=p_1=1/2$ and $\\text{Entropy} = 1$.\n",
    "    - When our dataset (or node) is \"pure\", say 0% of observations belonging to class \"0\" and 100% to class \"1\", then $p_0=0$, $p_1=1$, and $\\text{Entropy} = 0$\n",
    "\n",
    "<center>\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/800px-Binary_entropy_plot.svg.png' width=\"300\">\n",
    "</center>\n",
    "\n",
    "Source: Brona, Wikipedia [Binary entropy plot](https://commons.wikimedia.org/wiki/File:Binary_entropy_plot.svg)\n",
    "\n",
    "At each decision node, we compute the associated Entropy. This allows to calculate the information gain:\n",
    "$$\\text{Information Gain}=\\text{Entropy}_\\text{parent}-\\text{Average Entropy}_\\text{children}$$\n",
    "\n",
    "Our objective is to obtain pure leaf nodes, and thus to reduce the entropy in the children nodes. Say differently, we need to find the splits that maximize the Information Gain.\n",
    "\n",
    "- Gini Index, also called [Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), is an alternative decision criterion, inspired by the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient), a measure intended to represent the wealth inequality within a nation or a social group. The Gini of a dataset is:\n",
    "$$\\text{Gini}=1-\\sum_{i=1}^c p_i^2$$\n",
    "Let's again consider two classes for simplicity \"0\" and \"1\", $\\text{Gini}=1-p_0^2-p_1^2$\n",
    "    - When our dataset (or node) has 50% of observations belonging to class \"0\" and 50% belonging to class \"1\", then $p_0=p_1=1/2$ and $\\text{Gini} = 0.5$.\n",
    "    - When our dataset (or node) is \"pure\", say 0% of observations belonging to class \"0\" and 100% to class \"1\", then $p_0=0$, $p_1=1$, and $\\text{Gini} = 0$\n",
    "\n",
    "At each decision node we compute the associated Gini Index, and then the average Gini Index of the split. Our objective is to minimize the Gini Index.\n",
    "\n",
    "For further information on the topic, you can read the articles: \n",
    "- [Decision Trees Explained — Entropy, Information Gain, Gini Index, CCP Pruning](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c)\n",
    "- [Understanding the Gini Index and Information Gain in Decision Trees](https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed40bd-9056-405a-b89b-15e479752fd0",
   "metadata": {},
   "source": [
    "### Implementation  <a id = \"DT-Implementation\"></a>\n",
    "\n",
    "We will use the same dataset, trying to predict the type of fruits based on the same features. We are using the same training and test set for comparability between model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa75a7-714d-4128-8ab2-93730c5f7c79",
   "metadata": {},
   "source": [
    "#### Building, training, and evaluating our classifier <a id = \"DT-Building\"></a>\n",
    "\n",
    "We implement a Decision Tree algorithm to predict the class of our target variable using the **sklearn** module `DecisionTreeClassifier()` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "```\n",
    "\n",
    "We can specify various parameters:\n",
    "- `criterion`= determines how to measure the quality of a split: \"gini\" for Gini Impurity (default value), \"entropy or \"log_loss\" for Information Gain\n",
    "- `max_depth`= determines the depth of the tree, i.e., the amount of nodes we allow the tree to generate. If None (default value), then nodes are expanded until all leaves are pure or until all leaves contain less than a given number of samples.\n",
    "\n",
    "Please refer to the documentation for the full list of parameters.\n",
    "\n",
    "We will start by arbitrarily selecting a maximum depth of 5, and default values for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b53c4-939f-4ec8-a717-ca803af15489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "model_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 5)\n",
    "\n",
    "# Fit model\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# Test accuracy\n",
    "print('Accuracy of Decision Tree on test set: {:.2f}'.format(model_tree.score(X_test, y_test)))\n",
    "print('Accuracy of Decision Tree on training set: {:.2f}'.format(model_tree.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b200d-dfb8-46a2-971f-ea2ca3afd697",
   "metadata": {},
   "source": [
    "We achieve perfection on the test set, but our model may be overly complicated. Let's see if we can reduce max depth without losing accuracy by using cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95489c7f-e616-47a0-8908-8c26c2eca5e9",
   "metadata": {},
   "source": [
    "#### Tuning parameters with cross-validation <a id = \"DT-Tuning\"></a>\n",
    "\n",
    "We proceed as previously for the KNN algorithm, using Grid Search Cross Validation. \n",
    "\n",
    "The associated sklearn module is `GridSearchCV()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3b152-bab7-4733-be1c-a3488e970ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to test\n",
    "grid_tree = {'criterion':['gini','entropy'] ,     # criterion\n",
    "        'max_depth':np.arange(1,5),               # array from 1 to 5, maximum depth\n",
    "       }\n",
    "\n",
    "# Define and fit model\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree_cv = GridSearchCV(dec_tree, grid_tree, cv=5)\n",
    "dec_tree_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Hyperparameters:\", dec_tree_cv.best_params_)\n",
    "print(\"Best model:\", dec_tree_cv.best_estimator_)\n",
    "print(\"Train Score: {:0.2f}\".format(dec_tree_cv.best_score_))\n",
    "print(\"Test Score: {:0.2f}\".format(dec_tree_cv.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7badf-2b72-4728-b4fd-9c9dea66d933",
   "metadata": {},
   "source": [
    "We could reduce the depth to 2 and still got a perfect accuracy on the test set. However, we should remember that we have a limited number of observations so the results should be understood with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e13259-d288-4de5-9c2e-32e9c2127848",
   "metadata": {},
   "source": [
    "#### Visualize tree <a id = \"DT-Visualize\"></a>\n",
    "\n",
    "We can visualize our Decision Tree, allowing us to get a better understanding of the decisions made by our algorithm and of the features that played a key role in our classification.\n",
    "\n",
    "To do so, we are using the `plot_tree()` module of sklear ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "```\n",
    "\n",
    "We plot our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfead1-0fc6-4e52-805e-35974ad6c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plot_tree(model_tree, filled=True, feature_names=('mass', 'width', 'height', 'color_score'), fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c501ff5-145a-4758-95ea-27ffb42febe6",
   "metadata": {},
   "source": [
    "Our tree starts with the root in which we have 38 samples (our data points), 19 belonging to class 1 and 19 belonging to class 2.\n",
    "\n",
    "Each node represents a condition on which the tree splits into branches. For instance, the first two depth levels are using the \"color_score\" as feature to split, and the last node is using the mass. \n",
    "\n",
    "The end of a branch that no longer splits is a leaf. Here we have perfectly classified our observations. \n",
    "\n",
    "The colors represent the purity of a node. In our case, blue corresponds to class 1 (apples) and orange corresponds to class 2 (oranges). They are displayed because we specified the parameter `filled = True`.\n",
    "\n",
    "Finally, the gini coefficient is our measure of purity for each node. In our dataset we start with 0.5 (corresponds to the 50-50 distribution of classes in the root) and then gradually go down to 0 (maximum purity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8367747-4cbe-45e1-8003-b4ba277f2741",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f169d7-4221-4e1d-b62a-dae28963c139",
   "metadata": {},
   "source": [
    "Now it's your turn to implement a classifier! We will try to improve the accuracy we obtained last week when we implemented a logistic regression classifier to predict whether a forest fire spread and burned forest areas in the Montesinho natural park in Portugal.\n",
    "\n",
    "We are using the [Forest Fires dataset](https://www.kaggle.com/datasets/sumitm004/forest-fire-area), created by Paulo Cortez and Aníbal Morais, and available on Kaggle.\n",
    "\n",
    "Source: P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimaraes, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9.\n",
    "\n",
    "The original dataset contains 13 columns:\n",
    "- X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "- Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "- month - month of the year: \"jan\" to \"dec\" \n",
    "- day - day of the week: \"mon\" to \"sun\"\n",
    "- FFMC - Fine Fuel Moisture Code (FFMC) index from the [Fire Weather Index (FWI)](https://www.nwcg.gov/publications/pms437/cffdrs/fire-weather-index-system) system: 18.7 to 96.20\n",
    "- DMC - Duff Moisture Code (DMC) index from the FWI system: 1.1 to 291.3 \n",
    "- DC - Drought Code (DC) index from the FWI system: 7.9 to 860.6 \n",
    "- ISI - Initial Spread Index (ISI) index from the FWI system: 0.0 to 56.10\n",
    "- temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "- RH - relative humidity in %: 15.0 to 100\n",
    "- wind - wind speed in km/h: 0.40 to 9.40 \n",
    "- rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "- area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "\n",
    "In addition, we created a new column, \"class\", detailing whether the fire burned an area of forest:\n",
    "- class is equal to 0 if area = 0.00 ha\n",
    "- class is equal to 1 if area > 0.00 ha \n",
    "\n",
    "Our goal will be to predict the class using KNN algorithm and Decision Tree, given the weather and FWI features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63210f-c8c1-4c9d-a924-854e21252e44",
   "metadata": {},
   "source": [
    "Let's pick up where we left off last week, with a rescaled training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea817c8-e3db-42fd-a429-8804c19b528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "url_ff = 'https://raw.githubusercontent.com/thurmboris/Data-Science_4_Sustainability/main/data/forestfires.csv'\n",
    "forest_fire = pd.read_csv(url_ff)\n",
    "\n",
    "# Extract features and class\n",
    "X_forest = forest_fire[['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']]\n",
    "y_forest = forest_fire['class'] \n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_forest, y_forest, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=19, \n",
    "                                                            shuffle=True)\n",
    "\n",
    "# Rescaling\n",
    "scaler = MinMaxScaler() # Define the scaler\n",
    "scaler.fit(X_train)   # Fit the scaler\n",
    "# Transform the train and the test set\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d679a01-346f-4b45-91df-123fd2ddd013",
   "metadata": {},
   "source": [
    "Here is what we obtained with a Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11922362-4d2e-4733-a90c-8ada437f8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our model\n",
    "model_log = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Fit our model\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "print('Accuracy of classifier on test set: {:.2f}'\n",
    "     .format(model_log.score(X_test, y_test)))\n",
    "print('Accuracy of classifier on training set: {:.2f}'\n",
    "     .format(model_log.score(X_train, y_train)))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, model_log.predict(X_test)), annot=True, cmap='Blues', fmt='.4g')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac49cf1-ee80-430b-8a34-9f891e8442b7",
   "metadata": {},
   "source": [
    "- Build and train a KNN classifier with 5 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc882f09-0943-4969-9fc5-9d40e7e71659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Set up our model\n",
    "model_kNN_f = KNeighborsClassifier(n_neighbors=5, p=2, weights='uniform')\n",
    "\n",
    "# Fit our model\n",
    "model_kNN_f.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a25ac-f992-4e82-bf10-55f3f17e9f9c",
   "metadata": {},
   "source": [
    "- Compare the training and testing accuracy of your model, and create a heatmap of the confusion matrix. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13383d7f-a616-4a72-9889-d78227f68923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'.format(model_kNN_f.score(X_test, y_test)))\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'.format(model_kNN_f.score(X_train, y_train)))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, model_kNN_f.predict(X_test)), annot=True, cmap='Blues', fmt='.4g')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5559672-1a39-445a-9a4b-9b1b537dc428",
   "metadata": {},
   "source": [
    "- Explore a grid a parameters with `GridSearchCV`, using 7 folds. Check the accuracy on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3f8e3-84eb-4aa8-a377-5cdb89339a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Define parameters to test\n",
    "grid = {'n_neighbors':np.arange(1,10),     # array from 1 to 10 neighbors\n",
    "        'p':np.arange(1,3),                # array from 1 to 3, distance metrics\n",
    "        'weights':['uniform','distance']   # weights\n",
    "       }\n",
    "\n",
    "# Define and fit model\n",
    "knn_f = KNeighborsClassifier()\n",
    "knn_cv_f = GridSearchCV(knn_f, grid, cv=7)\n",
    "knn_cv_f.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Hyperparameters:\", knn_cv_f.best_params_)\n",
    "print(\"Train Score: {:0.2f}\".format(knn_cv_f.best_score_))\n",
    "print(\"Test Score: {:0.2f}\".format(knn_cv_f.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d48b11-4cee-4904-9adb-03d891b1adfa",
   "metadata": {},
   "source": [
    "- Build and train a Decision Tree Classifier with a maximum depth of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547a215-9365-4db4-8cf3-034ac336f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Create model \n",
    "model_tree_f = DecisionTreeClassifier(criterion = 'gini', max_depth = 7)\n",
    "\n",
    "# Fit model\n",
    "model_tree_f.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6f3c8-4d5c-4d63-a41d-6766f5e383a8",
   "metadata": {},
   "source": [
    "- Compare the accuracy on the training set and the test set, and create a heatmap of your confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000bf99-2eb4-4946-9ae9-9bad47af96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Test accuracy\n",
    "print('Accuracy of Decision Tree on test set: {:.2f}'.format(model_tree_f.score(X_test, y_test)))\n",
    "print('Accuracy of Decision Tree on training set: {:.2f}'.format(model_tree_f.score(X_train, y_train)))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, model_tree_f.predict(X_test)), annot=True, cmap='Blues', fmt='.4g')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60ea99-a687-49d2-8b6d-2790a521e59d",
   "metadata": {},
   "source": [
    "- Explore with `GridSearchCV` a grid of parameters. Check the accuracy on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17ec90-6fce-4073-a459-d859f8b66dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Define parameters to test\n",
    "grid_tree_f = {'criterion':['gini','entropy'] ,     # criterion\n",
    "        'max_depth':np.arange(1,10),               # array from 1 to 10, maximum depth\n",
    "       }\n",
    "\n",
    "# Define and fit model\n",
    "dec_tree_f = DecisionTreeClassifier()\n",
    "dec_tree_cv_f = GridSearchCV(dec_tree_f, grid_tree_f, cv=7)\n",
    "dec_tree_cv_f.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Hyperparameters:\", dec_tree_cv_f.best_params_)\n",
    "print(\"Best model:\", dec_tree_cv_f.best_estimator_)\n",
    "print(\"Train Score: {:0.2f}\".format(dec_tree_cv_f.best_score_))\n",
    "print(\"Test Score: {:0.2f}\".format(dec_tree_cv_f.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255445a-9c90-43cc-9520-1d5b3248ff06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
