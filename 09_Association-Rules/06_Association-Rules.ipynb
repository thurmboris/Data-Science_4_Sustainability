{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "# Import from mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984e553-36af-42c8-9522-dd090686c913",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*4yFCbNwp0gGdGR5KbquFHA.png' width=\"600\">\n",
    "\n",
    "Source: [Comparing Association Rule Mining with other similar methods](https://medium.com/@utkarsh.kant/comparing-association-rule-mining-with-other-similar-methods-d964eaafad91), by Utkarsh Kant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392fab0-671b-45ea-b27a-61a797c0e090",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "The goal of this walkthrough is to provide you with insights on association rules. After presenting the main concepts, you will be introduced to the techniques to implement association rule mining in Python. Finally, it will be your turn to practice, using an application on groceries purchase.\n",
    "\n",
    "This notebook is organized as follows:\n",
    "- [Background](#Background)\n",
    "    - [Objective](#Objective)\n",
    "    - [Concepts](#Concepts)\n",
    "    - [Apriori algorithm](#Apriori-algorithm)\n",
    "- [Implementation](#Implementation)\n",
    "    - [Discover dataset](#Discover-dataset)\n",
    "    - [Preprocessing](#Preprocessing)\n",
    "    - [Applying Apriori algorithm](#Applying-Apriori-algorithm)\n",
    "    - [Mining Association Rules](#Mining-Association-Rules)\n",
    "- [Your turn!](#Your-turn!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646c103-d6d2-4dc7-bcdf-3ea14d961a4b",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Objective\n",
    "\n",
    "[Association rule](https://en.wikipedia.org/wiki/Association_rule_learning) aims at discovering interesting relations between variables in large dataset. Like clustering, association rule mining is an **unsupervised learning** method. However, while clustering techniques calculate clusters based on similarities, association rule finds associations based on co-occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5570d-cf11-4ef6-90cd-f5bdef3a8d23",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n",
    "Our goal is to learn a rule $\\Rightarrow$ informing us that, when a set of items $S$ *occur together*, another item $i$ *frequently occurs with them*: $S \\Rightarrow i$. Note that **$\\Rightarrow$ does not indicate a causal link**.\n",
    "\n",
    "The most important relationships can be identified using the *support* and *confidence*:\n",
    "- The **support** indicates how frequently the itemset appears in our dataset, i.e., it measures the notion *occur together*:\n",
    "$$\\text{support}_{S \\Rightarrow i}=\\frac{\\text{# observations containing }S\\text{ and }i}{\\text{total number of observations}}$$\n",
    "- The **confidence** measures how frequently item $i$ appears with the set of items $S$, i.e., the notion *frequently occurs with them*:\n",
    "$$\\text{confidence}_{S \\Rightarrow i}=\\frac{\\text{# observations containing }S\\text{ and }i}{\\text{# observations containing }S}$$\n",
    "\n",
    "We need both the support and confidence to satisfy a minimum *threshold*. Indeed:\n",
    "- a low support indicates that the relation can happen by chance and may not be generalized.\n",
    "- a low confidence indicates that the rule is not reliable.\n",
    "\n",
    "One drawback of the confidence is that $S \\Rightarrow i$ can have a high confidence because item $i$ appears frequently, not because it is associated with $S$. To better measure the interestingness of a rule, we can use the **lift**:\n",
    "$$\\text{lift}_{S \\Rightarrow i}=\\frac{\\frac{\\text{# observations containing }S\\text{ and }i}{\\text{# observations containing }S}}{\\frac{\\text{# observations containing }i}{\\text{# total observations}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29353965-f193-4689-93ee-100769b34876",
   "metadata": {},
   "source": [
    "### Apriori algorithm\n",
    "\n",
    "[Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm) is an algorithm for frequent item set mining and association rule learning, proposed by Agrawal and Srikant in 1994.\n",
    "\n",
    "The main idea of Apriori is that the subsets of a frequent itemset must also be frequent.\n",
    "$$\\text{For all sets } X,Y, \\text{ if } (X \\subseteq Y) \\text{ then support}(X) \\geq \\text{support}(Y) $$\n",
    "Reciprocally, if a itemset is not frequent, then its supersets cannot be frequent.\n",
    "\n",
    "Hence, instead of computing the support of each itemset, which would be computationally expensive, Apriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time and tested, while infrequent itemset and all their supersets are pruned, i.e., not considered.\n",
    "\n",
    "*Reference:* Agrawal, Rakesh, and Ramakrishnan Srikant. \"[Fast algorithms for mining association rules](https://www.it.uu.se/edu/course/homepage/infoutv/ht08/vldb94_rj.pdf)\" Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a19ea-246b-4af3-bd04-885218a6ccff",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a30116-9a25-4ea2-aaf6-891fe1bcd6bb",
   "metadata": {},
   "source": [
    "We will implement the Apriori algorithm to mine the frequent itemsets. The `mlxtend` library has an implementation of this algorithm [Documentation](http://rasbt.github.io/mlxtend/). You can install the library using `pip` or `conda`:\n",
    "\n",
    "```python\n",
    "!pip install mlxtend\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4274f-e942-4422-be1b-22805b54d3aa",
   "metadata": {},
   "source": [
    "### Discover dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058ae59-5ad6-48c6-9d28-8e635e6d09b4",
   "metadata": {},
   "source": [
    "We are going to use a dataset containing the purchase of customers, available in the /data folder of the course repository.\n",
    "\n",
    "Source: [Harsh-Git-Hub](https://gist.github.com/Harsh-Git-Hub/2979ec48043928ad9033d8469928e751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986647fb-6855-4579-bfdd-11c3bf975224",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_retail ='https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/retail.csv'\n",
    "retail = pd.read_csv(url_retail, sep=',')\n",
    "retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423693b-3697-4d1e-b3bf-835f1cad14a5",
   "metadata": {},
   "source": [
    "Each row of the dataset represents items that were purchased together by a customer, on the same day at the same store.\n",
    "\n",
    "The dataset is **sparse**, as a relatively high percentage of cells is null (NA, NaN or equivalent). These null values make it difficult to read the table. Let's find out which unique items can actually be found in the table (based on the first column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14f6a0-f75e-4388-b172-bbae2d950c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique items in first column:\n",
    "items = retail['0'].unique()\n",
    "\n",
    "# Print result - we use the join method to print items one by one:\n",
    "print('Our dataset contains the following items: '\n",
    "      +', '.join(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb63b6-9085-4554-bdbf-2d0d7bafb37b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "To make use of the `apriori` module given by `mlxtend` library, we need to convert the dataset to the appropriate format. The `apriori` module requires a dataframe that has either 0 and 1 or True and False as data. Since the data we have is all strings (names of items), we need to encode the data.\n",
    "\n",
    "We first convert our dataframe to a list of list, removing the NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b96cdf-0a1e-49ba-8f23-69473da8523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to list of list\n",
    "retail_list = retail.values.tolist()\n",
    "\n",
    "# Remove NaNs with list comprehensions\n",
    "retail_list_cleaned = [[x for x in y if str(x) != 'nan'] for y in retail_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875cbb2f-0466-45f6-a340-2928873d96ae",
   "metadata": {},
   "source": [
    "Let's check the results for a few transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc49ff-2f48-4fb8-b1fd-fbc597c8515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retail_list_cleaned[0])\n",
    "print(retail_list_cleaned[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229efd2-6250-45df-8dbf-f471bc2cad12",
   "metadata": {},
   "source": [
    "Next, we use the `TransactionEncoder` module of the `mlxtend` library to transform the transactions to `True` or `False` ([Documentation](http://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/)). The module is imported at the beginning of the notebook with the following line of code:\n",
    "\n",
    "```python\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c742bd30-5a9c-44db-87ce-4ab8cd1a88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of Encoder\n",
    "te = TransactionEncoder()\n",
    "\n",
    "# Fit encoder and transform our list\n",
    "retail_list_encoded = te.fit(retail_list_cleaned).transform(retail_list_cleaned)\n",
    "\n",
    "# Create dataframe with results\n",
    "retail_encoded = pd.DataFrame(retail_list_encoded, columns=te.columns_)\n",
    "retail_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a39bf9-fb5b-4100-a46d-3bdca5210be3",
   "metadata": {},
   "source": [
    "### Applying Apriori algorithm\n",
    "\n",
    "We will now implement the Apriori algorithm using the `apriori` module of the `mlxtend` library to find the frequent itemsets ([Documentation](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "```\n",
    "\n",
    "Here are some of the parameters of the module:\n",
    "- `df` : DataFrame that has 0 and 1 or True and False as values\n",
    "- `min_support` : Floating point value between 0 and 1 that indicates the minimum support required for an itemset to be selected.\n",
    "- `use_colnames` : Allows to preserve column names for itemset making it more readable.\n",
    "- `max_len` : Max length of itemset generated. If not set, all possible lengths are evaluated.\n",
    "\n",
    "As output, we obtain a DataFrame with columns 'support' and 'itemsets' of all itemsets that have a support greater than `min_support` and a length strictly lower than `max_len`.\n",
    "\n",
    "Let's try with a minimum support of 0.2 and no maximum length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b97b7-8568-4f15-b2cb-cad0567890fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori algorithm\n",
    "freq_items = apriori(retail_encoded, min_support=0.2, use_colnames=True)\n",
    "freq_items.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3e93e-2aff-4126-904a-1e47d0226970",
   "metadata": {},
   "source": [
    "### Mining Association Rules\n",
    "\n",
    "We will now mine association rules using the `association_rules` module of the `mlxtend` library  ([Documentation](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)). Here is the import line:\n",
    "\n",
    "```python\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "```\n",
    "\n",
    "As you know by now, frequent if-then associations are called \"association rules\". They consist of an antecedent (if) and a consequent (then): `{antecedent} => {consequent}`.\n",
    "\n",
    "The `association_rules` module requires as input parameters a DataFrame of frequent itemsets as well as:\n",
    "- `metric` : metric to evaluate if a rule is of interest; can be set to \"support\", \"confidence\", \"lift\", \"leverage\" and \"conviction\". See the documentation for more information on how these metrics are defined.\n",
    "- `min_threshold` : minimal threshold for the evaluation metric to decide whether a candidate rule is of interest.\n",
    "\n",
    "We obtain as output a DataFrame with columns \"antecedents\" and \"consequents\" that store itemsets, plus the scoring metric columns: \"antecedent support\", \"consequent support\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\" of all rules for which the `metric` is greater than the `min_thresold`. \n",
    "\n",
    "Let's try using the the confidence metric with a threshold of 0.6, i.e., we are only keeping rules with a confidence at or above 0.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d39320-31be-4393-9e8c-baa46895f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate rules\n",
    "rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# Rules sorted by lift\n",
    "rules.sort_values(by=\"lift\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f862c63-3ba9-4284-bd65-51fd3220afe7",
   "metadata": {},
   "source": [
    "The `rules` dataframe contains all the association rules that we determined as interesting. What do you think? Are they really interesting? What does the lift metric tells us?\n",
    "\n",
    "Use the interactive function below to further explore the above rules with different threshold for confidence. What do you think about the rules when the threshold is 0.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc81ead7-4f86-4f40-aef3-5192bab71263",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['lift', 'support', 'confidence']\n",
    "thresholds = [0.6, 0.5, 0.4]\n",
    "\n",
    "@interact\n",
    "def interactive_association(sort_by = metrics, threshold = thresholds):\n",
    "    rules_interactive = association_rules(freq_items, metric=\"confidence\", min_threshold= threshold)\n",
    "    return rules_interactive.sort_values(by=sort_by, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bfaa9-e4eb-4f1e-8752-2edb0f4b5cef",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Now it's your turn to practice. We will use a bigger dataset containing the groceries purchase of customers. \n",
    "\n",
    "Note that this is not a proper CSV file since there are different number of values in each row. Hence, we have to read the file manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f559ea-e11d-4bb8-957a-a04f151234e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_groceries = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/groceries.csv'\n",
    "\n",
    "# Open and read url, and decode into a string\n",
    "groceries_str = urllib.request.urlopen(url_groceries).read().decode(\"utf-8\")\n",
    "\n",
    "# Create a list where each item is one line, i.e., one transaction\n",
    "groceries_lis = groceries_str.split('\\n')\n",
    "\n",
    "# Create a list of list where each item is one good\n",
    "groceries = [[item for item in line.split(',')] for line in groceries_lis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128f949-6a89-4022-ae5e-d5509ff18b75",
   "metadata": {},
   "source": [
    "Here is how our processed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193b9a6-9d06-44c3-b525-de244f2e7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a26885-ae1b-4cb6-a250-517ca5530203",
   "metadata": {},
   "source": [
    "- Encode the data in a dataframe of True and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805ca40-0726-4057-bede-10cae05f3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b93bc-ba78-4342-bef9-1045cb010965",
   "metadata": {},
   "source": [
    "- Find association rules for the Groceries dataset using **confidence** as the `metric` parameter, a support threshold of **0.001** and confidence threshold of **0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abe2ea-d666-45c2-bb3c-62e6a55eaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56596f-d6b2-4bac-9022-00a19081afd2",
   "metadata": {},
   "source": [
    "- Extract all the rules you have found containing \"bottled beer\" as *antecedent*. Which rules do you find interesting? Can you explain them (e.g., potato chips may be frequently bought with bottled bears for \"apéro\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed03f80-5047-4fe9-a96e-47b828471eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558eb4ec-f72d-4be7-b091-952bb4209a67",
   "metadata": {},
   "source": [
    "- Feel free to further explore various other thresholds and antecedents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf09c4-ad66-43a0-bb93-131b4b131dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
